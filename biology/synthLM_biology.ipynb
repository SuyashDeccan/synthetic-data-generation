{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n",
      "Warning: could not load bio_docs/16.pdf: No /Root object! - Is this really a PDF?\n",
      "Warning: could not load bio_docs/17.pdf: No /Root object! - Is this really a PDF?\n",
      "Warning: could not load bio_docs/29.pdf: No /Root object! - Is this really a PDF?\n",
      "Warning: could not load bio_docs/15.pdf: No /Root object! - Is this really a PDF?\n",
      "Warning: could not load bio_docs/14.pdf: No /Root object! - Is this really a PDF?\n",
      "Warning: could not load bio_docs/28.pdf: No /Root object! - Is this really a PDF?\n",
      "Warning: could not load bio_docs/38.pdf: No /Root object! - Is this really a PDF?\n",
      "Warning: could not load bio_docs/39.pdf: No /Root object! - Is this really a PDF?\n",
      "Warning: could not load bio_docs/11.pdf: No /Root object! - Is this really a PDF?\n",
      "Warning: could not load bio_docs/12.pdf: No /Root object! - Is this really a PDF?\n",
      "Warning: could not load bio_docs/23.pdf: No /Root object! - Is this really a PDF?\n",
      "Warning: could not load bio_docs/37.pdf: No /Root object! - Is this really a PDF?\n",
      "Warning: could not load bio_docs/36.pdf: No /Root object! - Is this really a PDF?\n",
      "Warning: could not load bio_docs/22.pdf: No /Root object! - Is this really a PDF?\n",
      "Warning: could not load bio_docs/34.pdf: No /Root object! - Is this really a PDF?\n",
      "Warning: could not load bio_docs/20.pdf: No /Root object! - Is this really a PDF?\n",
      "Warning: could not load bio_docs/21.pdf: No /Root object! - Is this really a PDF?\n",
      "Warning: could not load bio_docs/35.pdf: No /Root object! - Is this really a PDF?\n",
      "Warning: could not load bio_docs/31.pdf: No /Root object! - Is this really a PDF?\n",
      "Warning: could not load bio_docs/25.pdf: No /Root object! - Is this really a PDF?\n",
      "Warning: could not load bio_docs/19.pdf: No /Root object! - Is this really a PDF?\n",
      "Warning: could not load bio_docs/18.pdf: No /Root object! - Is this really a PDF?\n",
      "Warning: could not load bio_docs/24.pdf: No /Root object! - Is this really a PDF?\n",
      "Warning: could not load bio_docs/30.pdf: No /Root object! - Is this really a PDF?\n",
      "Warning: could not load bio_docs/26.pdf: No /Root object! - Is this really a PDF?\n",
      "Warning: could not load bio_docs/32.pdf: No /Root object! - Is this really a PDF?\n",
      "Warning: could not load bio_docs/33.pdf: No /Root object! - Is this really a PDF?\n",
      "Warning: could not load bio_docs/27.pdf: No /Root object! - Is this really a PDF?\n",
      "Warning: could not load bio_docs/40.pdf: No /Root object! - Is this really a PDF?\n",
      "Warning: could not load bio_docs/41.pdf: No /Root object! - Is this really a PDF?\n",
      "Loaded 3 documents\n",
      "\n",
      "Extracting topics and key concepts...\n",
      "Raw output: {\n",
      "  \"topics\": [\n",
      "    \"Maxillomandibular Fixation (MMF) Techniques\",\n",
      "    \"Types of MMF Appliances (Erich’s arch bar, MMF screws, hybrid arch bar)\",\n",
      "    \"Comparative Assessment of Efficiency and Patient Tolerance\",\n",
      "    \"Gingival Inflammation and Oral Hygiene Management\",\n",
      "    \"Time Efficiency in Application and Removal of MMF\",\n",
      "    \"Patient Discomfort and Satisfaction\",\n",
      "    \"Complications and Challenges in MMF\",\n",
      "    \"Historical Development and Evolution of Jaw Fracture Management\",\n",
      "    \"Indications and Contraindications for Different MMF Methods\",\n",
      "    \"Cost Analysis and Practical Considerations in MMF\"\n",
      "  ],\n",
      "  \"key_concepts\": [\n",
      "    \"Erich’s arch bar: traditional stainless steel appliance for MMF with longer placement time and moderate gingival inflammation\",\n",
      "    \"MMF screws: bone-anchored, quicker application and removal, least gingival inflammation, highest patient comfort\",\n",
      "    \"Hybrid arch bar: combination of MMF screws and arch bar, shorter application time than Erich’s arch bar but higher gingival inflammation\",\n",
      "    \"Gingival inflammation: higher in hybrid arch bar immediately post-removal, decreases after 7 days\",\n",
      "    \"Time metrics: MMF screws fastest in both application and removal, Erich’s arch bar slowest\",\n",
      "    \"Patient discomfort: lowest with MMF screws, higher with Erich’s arch bar and hybrid arch bar\",\n",
      "    \"Complications: include mucosal overgrowth, screw loosening, injury to tooth roots, and difficulties in maintaining oral hygiene\",\n",
      "    \"Study methodology: prospective, single-blind, comparative clinical trial with three groups\",\n",
      "    \"Clinical outcomes: efficiency favoring MMF screws, moderate in hybrid arch bars, and least in Erich’s arch bar in some parameters\",\n",
      "    \"Clinical recommendations: MMF screws preferred in short-term MMF, Erich’s arch bar in long-term stabilization, hybrid arch in partially edentulous cases\"\n",
      "  ]\n",
      "}\n",
      "Successfully extracted 10 topics and 10 key concepts from 76833_CE[Ra1]_F(SS)_QC_PF1(HJ_SS)_PFA(IS)_PN(IS).pdf\n",
      "Raw output: {\n",
      "  \"topics\": [\n",
      "    \"Endometrial Carcinoma and Hyperplasia\",\n",
      "    \"Immunohistochemical Markers in Endometrial Lesions\",\n",
      "    \"Role of PAX2 in Endometrial Pathology\",\n",
      "    \"Molecular Pathogenesis of Endometrial Cancer\",\n",
      "    \"Diagnostic and Prognostic Implications of PAX2 Expression\",\n",
      "    \"Histopathological Classification of Endometrial Lesions\",\n",
      "    \"Clinicopathological Parameters in Endometrial Cancer\",\n",
      "    \"Potential Therapeutic Targets in Endometrial Cancer\"\n",
      "  ],\n",
      "  \"key_concepts\": [\n",
      "    \"PAX2 as a transcription factor involved in embryogenesis and carcinogenesis\",\n",
      "    \"Decreased PAX2 expression correlates with progression from normal endometrium to hyperplasia and adenocarcinoma\",\n",
      "    \"Loss of PAX2 expression associated with advancing stage of endometrial carcinoma\",\n",
      "    \"Immunohistochemical evaluation of PAX2 in different endometrial tissues\",\n",
      "    \"Significance of PAX2 in differentiating benign from precancerous and malignant endometrial conditions\",\n",
      "    \"Lack of association between PAX2 expression and tumor size, grade, or invasion, except for stage\",\n",
      "    \"Potential of PAX2 as a diagnostic marker and therapeutic target in endometrial carcinoma\",\n",
      "    \"Limitations include small sample size and need for longitudinal follow-up for prognostic validation\"\n",
      "  ]\n",
      "}\n",
      "Successfully extracted 8 topics and 8 key concepts from 76047_CE[Ra1]_F(IS)_QC(AN_SS)_PF1(VD_SS)_redo_PFA(IS)_PB(VD_IS)_PN(IS).pdf\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 355\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# 1) Extract topics & KCs\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExtracting topics and key concepts...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 355\u001b[0m extractions \u001b[38;5;241m=\u001b[39m \u001b[43mextract_concepts_from_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# Save extractions to file\u001b[39;00m\n\u001b[1;32m    358\u001b[0m save_extractions(extractions, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument_extractions.json\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[2], line 71\u001b[0m, in \u001b[0;36mextract_concepts_from_docs\u001b[0;34m(doc_texts, filenames, model)\u001b[0m\n\u001b[1;32m     65\u001b[0m filename \u001b[38;5;241m=\u001b[39m filenames[i] \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(filenames) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m prompt \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtract high-level topics and key concepts from the following document. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturn JSON with keys \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopics\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey_concepts\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m )\n\u001b[0;32m---> 71\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are an expert summarizer. Return your response as a JSON object with \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtopics\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m and \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkey_concepts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m as arrays.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m output \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw output:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/resources/chat/completions.py:579\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    577\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    578\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1239\u001b[0m     )\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py:952\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    949\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 952\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    958\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpx/_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpx/_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    951\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpx/_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    989\u001b[0m     hook(request)\n\u001b[0;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpx/_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1024\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1031\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpx/_transports/default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    241\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    242\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    243\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    244\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    245\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py:1226\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1223\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1224\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1225\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py:1101\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "import pdfplumber\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key from environment variable\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# -- Document Loading ------------------------------------------------------\n",
    "\n",
    "def load_docs_from_dir(dir_path):\n",
    "    \"\"\"\n",
    "    Load all .pdf files from a directory and extract their full text.\n",
    "    Returns a list of strings, one per PDF.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    filenames = []\n",
    "    for filepath in glob.glob(os.path.join(dir_path, '*.pdf')):\n",
    "        try:\n",
    "            text_pages = []\n",
    "            with pdfplumber.open(filepath) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    text_pages.append(page.extract_text() or \"\")\n",
    "            full_text = \"\\n\".join(text_pages)\n",
    "            docs.append(full_text)\n",
    "            filenames.append(os.path.basename(filepath))\n",
    "        except Exception as e:\n",
    "            # skip unreadable PDFs\n",
    "            print(f\"Warning: could not load {filepath}: {e}\")\n",
    "    return docs, filenames\n",
    "\n",
    "# -- 1. Extract topics and key concepts -----------------------------------\n",
    "\n",
    "def extract_json_from_text(text):\n",
    "    \"\"\"Helper function to extract JSON content from markdown code blocks or raw text\"\"\"\n",
    "    # Try to extract JSON from markdown code blocks\n",
    "    json_match = re.search(r'```(?:json)?\\s*(.*?)\\s*```', text, re.DOTALL)\n",
    "    if json_match:\n",
    "        return json_match.group(1).strip()\n",
    "    \n",
    "    # If no code blocks, try to find JSON-like structures\n",
    "    json_match = re.search(r'(\\{.*\\})', text, re.DOTALL)\n",
    "    if json_match:\n",
    "        return json_match.group(1).strip()\n",
    "    \n",
    "    # Return original text if no JSON structure found\n",
    "    return text\n",
    "\n",
    "def extract_concepts_from_docs(doc_texts, filenames, model=\"gpt-4.1-nano\"):  \n",
    "    \"\"\"\n",
    "    Call OpenAI API to extract topics and key concepts from each document.\n",
    "    Returns list of dicts with keys 'topics' and 'key_concepts'.\n",
    "    \"\"\"\n",
    "    extractions = []\n",
    "    for i, text in enumerate(doc_texts):\n",
    "        filename = filenames[i] if i < len(filenames) else f\"doc_{i}\"\n",
    "        \n",
    "        prompt = (\n",
    "            \"Extract high-level topics and key concepts from the following document. \"\n",
    "            f\"Return JSON with keys 'topics' and 'key_concepts'.\\n\\n{text}\"\n",
    "        )\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert summarizer. Return your response as a JSON object with 'topics' and 'key_concepts' as arrays.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        output = response.choices[0].message.content\n",
    "        print(\"Raw output:\", output)\n",
    "        \n",
    "        try:\n",
    "            # Clean and extract JSON from the output\n",
    "            json_str = extract_json_from_text(output)\n",
    "            data = json.loads(json_str)\n",
    "            \n",
    "            # Ensure we have the expected keys\n",
    "            if \"topics\" in data and \"key_concepts\" in data:\n",
    "                extraction = {\n",
    "                    \"filename\": filename,\n",
    "                    \"topics\": data[\"topics\"],\n",
    "                    \"key_concepts\": data[\"key_concepts\"]\n",
    "                }\n",
    "                extractions.append(extraction)\n",
    "                print(f\"Successfully extracted {len(data['topics'])} topics and {len(data['key_concepts'])} key concepts from {filename}\")\n",
    "            else:\n",
    "                print(f\"Warning: Parsed JSON doesn't have expected keys: {data.keys()}\")\n",
    "                extractions.append({\n",
    "                    \"filename\": filename,\n",
    "                    \"topics\": [], \n",
    "                    \"key_concepts\": []\n",
    "                })\n",
    "                \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Failed to parse JSON: {e}\")\n",
    "            extractions.append({\n",
    "                \"filename\": filename,\n",
    "                \"topics\": [], \n",
    "                \"key_concepts\": []\n",
    "            })\n",
    "    \n",
    "    return extractions\n",
    "\n",
    "# -- 2. Construct the concept graph ----------------------------------------\n",
    "\n",
    "def build_concept_graph(extractions, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Build a unified graph G where nodes are topics + key concepts and\n",
    "    edges weighted by log(freq+eps) based on co-occurrence in docs.\n",
    "    Returns: G (nx.Graph), topic_nodes, kc_nodes\n",
    "    \"\"\"\n",
    "    freq = Counter()\n",
    "    all_topics, all_kcs = set(), set()\n",
    "\n",
    "    for ex in extractions:\n",
    "        nodes = ex[\"topics\"] + ex[\"key_concepts\"]\n",
    "        for i in range(len(nodes)):\n",
    "            for j in range(i+1, len(nodes)):\n",
    "                u, v = sorted((nodes[i], nodes[j]))\n",
    "                freq[(u, v)] += 1\n",
    "        all_topics.update(ex[\"topics\"])\n",
    "        all_kcs.update(ex[\"key_concepts\"])\n",
    "\n",
    "    G = nx.Graph()\n",
    "    for (u, v), f in freq.items():\n",
    "        weight = math.log(f + eps)\n",
    "        G.add_edge(u, v, weight=weight)\n",
    "\n",
    "    print(f\"Graph built with {len(all_topics)} topics and {len(all_kcs)} key concepts\")\n",
    "    return G, all_topics, all_kcs\n",
    "\n",
    "# -- Helpers for sampling --------------------------------------------------\n",
    "\n",
    "def softmax(weights):\n",
    "    exps = [math.exp(w) for w in weights]\n",
    "    s = sum(exps) or 1.0\n",
    "    return [e/s for e in exps]\n",
    "\n",
    "\n",
    "def random_walk(G, start, steps):\n",
    "    \"\"\"\n",
    "    Random walk on graph G for given steps from 'start',\n",
    "    with transition probabilities via softmax over edge weights.\n",
    "    \"\"\"\n",
    "    path = [start]\n",
    "    current = start\n",
    "    for _ in range(steps):\n",
    "        nbrs = list(G[current])\n",
    "        if not nbrs:\n",
    "            break\n",
    "        weights = [G[current][n]['weight'] for n in nbrs]\n",
    "        probs = softmax(weights)\n",
    "        current = random.choices(nbrs, probs)[0]\n",
    "        path.append(current)\n",
    "    return path\n",
    "\n",
    "# -- 3. Concept combination sampling ---------------------------------------\n",
    "\n",
    "def sample_concept_combinations(\n",
    "    G, topic_nodes, kc_nodes,\n",
    "    num_samples=100,\n",
    "    topic_walk_steps=(1, 2),\n",
    "    kc_walk_steps=(3, 4)\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate sampled sets of topics and key concepts via multi-stage random walks.\n",
    "    Returns list of dicts: {'topics': set, 'key_concepts': set}\n",
    "    \"\"\"\n",
    "    # Safety check\n",
    "    if not topic_nodes:\n",
    "        print(\"Error: No topics found. Cannot sample combinations.\")\n",
    "        return []\n",
    "        \n",
    "    G_topic = G.subgraph(topic_nodes)\n",
    "    G_topic_kc = G.subgraph(topic_nodes | kc_nodes)\n",
    "    G_kc = G.subgraph(kc_nodes)\n",
    "    samples = []\n",
    "\n",
    "    topics_list = list(topic_nodes)\n",
    "    print(f\"Sampling from {len(topics_list)} topics\")\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        t0 = random.choice(topics_list)\n",
    "        t_steps = random.choice(topic_walk_steps)\n",
    "        topic_path = random_walk(G_topic, t0, t_steps)\n",
    "        sampled_topics = set(topic_path)\n",
    "\n",
    "        kc_cands = [nbr for t in sampled_topics for nbr in G_topic_kc[t] if nbr in kc_nodes]\n",
    "        if kc_cands:\n",
    "            k0 = random.choice(kc_cands)\n",
    "            k_steps = random.choice(kc_walk_steps)\n",
    "            kc_path = random_walk(G_kc, k0, k_steps)\n",
    "            sampled_kcs = set(kc_path)\n",
    "        else:\n",
    "            sampled_kcs = set()\n",
    "\n",
    "        samples.append({\n",
    "            \"topics\": list(sampled_topics),  # Convert sets to lists for JSON serialization\n",
    "            \"key_concepts\": list(sampled_kcs)\n",
    "        })\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# -- 4. Question generation ------------------------------------------------\n",
    "\n",
    "def generate_questions_for_samples(combos, docs, extractions, model=\"gpt-4o\"):  \n",
    "    \"\"\"\n",
    "    For each sampled combo, pick two docs via Jaccard on concept sets,\n",
    "    then call LLM to generate questions.\n",
    "    Returns list of dicts: {'sample': combo, 'questions': [...]}.\n",
    "    \"\"\"\n",
    "    doc_concepts = [set(ex['topics'] + ex['key_concepts']) for ex in extractions]\n",
    "    # doc_concepts= doc_concepts[:1]\n",
    "    results = []\n",
    "    max_samples = 100\n",
    "    # combos = combos[:max_samples]\n",
    "    for i, combo in enumerate(combos):\n",
    "        combo_id = f\"combo_{i+1}\"\n",
    "        kg = set(combo['topics']) | set(combo['key_concepts'])\n",
    "        sims = []\n",
    "        for idx, dc in enumerate(doc_concepts):\n",
    "            inter = kg & dc\n",
    "            union = kg | dc\n",
    "            sims.append((len(inter) / (len(union) or 1), idx))\n",
    "        sims.sort(reverse=True)\n",
    "        top_idxs = [i for _, i in sims[:2]]\n",
    "        refs = [docs[i] for i in top_idxs]\n",
    "        ref_files = [extractions[i][\"filename\"] for i in top_idxs]\n",
    "        System_prompt =f\"\"\"\n",
    "Each question must follow these instructions:\n",
    "Model a Physical Scenario: Start from a real-world or idealized setup. Avoid abstract biology problems or purely conceptual statements.\n",
    "Target a Solvable Quantity: Ask for a clear symbolic expression of a physical variable (e.g., tension, acceleration, energy).\n",
    "Force Multi-Step Reasoning: Ensure the question involves a sequence of biology laws, transformations, and derivations to reach the answer.\n",
    "Avoid Redundancy: Exclude extraneous details or variables that do not impact the final solution.\n",
    "Be Unique: Do not rephrase standard textbook problems; ensure originality and complexity.\n",
    "Single solution: Expect a single symbolic expression, unambiguous, presented in LaTeX. Multiple equivalent algebraic forms are allowed. No equations or floating-point approximations.\n",
    "Use rigorous, concise phrasing.\n",
    "Avoid colloquial or ambiguous terminology.\n",
    "Units must be consistent; symbols should follow standard notation.\n",
    "\"\"\"\n",
    "        prompt = (\n",
    "            f\"Generate a set of difficult biology questions based on the following:\\n\"\n",
    "            \n",
    "            f\"Topics: {combo['topics']}\\n\"\n",
    "            f\"Key Concepts: {combo['key_concepts']}\\n\"\n",
    "            f\"Reference Doc 1:\\n{refs[0]}\\n\"\n",
    "        )\n",
    "        if len(refs) > 1:\n",
    "            prompt += f\"Reference Doc 2:\\n{refs[1]}\\n\"\n",
    "        prompt += \"Return a JSON array of questions.\"\n",
    "\n",
    "        # from ollama import chat\n",
    "        # from ollama import ChatResponse\n",
    "\n",
    "        # response: ChatResponse = chat(model='qwen3:8b', \n",
    "        #                                messages=[\n",
    "        #         {\"role\": \"system\", \"content\": System_prompt},\n",
    "        #         {\"role\": \"user\", \"content\": prompt}\n",
    "        #     ])\n",
    "        # # print(response['message']['content'])\n",
    "        # # or access fields directly from the response object\n",
    "        # print(response.message.content)\n",
    "        # content = response.message.content\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": System_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        try:\n",
    "            # Clean and extract JSON from the output\n",
    "            json_str = extract_json_from_text(content)\n",
    "            questions = json.loads(json_str)\n",
    "            if not isinstance(questions, list):\n",
    "                # If the output is an object with a questions key\n",
    "                if isinstance(questions, dict) and \"questions\" in questions:\n",
    "                    questions = questions[\"questions\"]\n",
    "                else:\n",
    "                    questions = [str(questions)]\n",
    "        except json.JSONDecodeError:\n",
    "            questions = [content]\n",
    "\n",
    "        results.append({\n",
    "            \"id\": combo_id,\n",
    "            \"topics\": combo['topics'],\n",
    "            \"key_concepts\": combo['key_concepts'],\n",
    "            \"reference_files\": ref_files,\n",
    "            \"questions\": questions\n",
    "        })\n",
    "        max_samples-=1\n",
    "        if max_samples == 0:\n",
    "            break\n",
    "\n",
    "\n",
    "    return results\n",
    "\n",
    "# -- Save outputs to files ------------------------------------------------\n",
    "\n",
    "def save_extractions(extractions, output_file=\"document_extractions.json\"):\n",
    "    \"\"\"Save the extracted topics and key concepts for each document\"\"\"\n",
    "    # Ensure the extractions are serializable (convert sets to lists)\n",
    "    serializable_extractions = []\n",
    "    for ex in extractions:\n",
    "        serializable_extractions.append({\n",
    "            \"filename\": ex[\"filename\"],\n",
    "            \"topics\": list(ex[\"topics\"]),\n",
    "            \"key_concepts\": list(ex[\"key_concepts\"])\n",
    "        })\n",
    "    \n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(serializable_extractions, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved document extractions to {output_file}\")\n",
    "\n",
    "def save_questions_with_topics(questions, output_file=\"questions_with_topics.json\"):\n",
    "    \"\"\"Save the generated questions with their topic combinations\"\"\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(questions, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved questions with topic combinations to {output_file}\")\n",
    "\n",
    "# -- Main Execution --------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = \"output_biology\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # adjust this path to where your .pdf docs live\n",
    "    docs_dir = \"bio_docs/\"\n",
    "    \n",
    "    print(\"Loading documents...\")\n",
    "    docs, filenames = load_docs_from_dir(docs_dir)\n",
    "    print(f\"Loaded {len(docs)} documents\")\n",
    "    \n",
    "    if not docs:\n",
    "        print(\"No documents found. Please check the docs directory.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # 1) Extract topics & KCs\n",
    "    print(\"\\nExtracting topics and key concepts...\")\n",
    "    extractions = extract_concepts_from_docs(docs, filenames)\n",
    "    \n",
    "    # Save extractions to file\n",
    "    save_extractions(extractions, os.path.join(output_dir, \"document_extractions.json\"))\n",
    "    \n",
    "    # Verify we have valid extractions\n",
    "    valid_extractions = [ex for ex in extractions if ex[\"topics\"] or ex[\"key_concepts\"]]\n",
    "    if not valid_extractions:\n",
    "        print(\"No valid topics or key concepts extracted. Check your data and API responses.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # 2) Build graph\n",
    "    print(\"\\nBuilding concept graph...\")\n",
    "    G, topic_nodes, kc_nodes = build_concept_graph(extractions)\n",
    "    \n",
    "    if not topic_nodes:\n",
    "        print(\"No topics found in the graph. Cannot proceed.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # 3) Sample combinations\n",
    "    print(\"\\nSampling concept combinations...\")\n",
    "    combos = sample_concept_combinations(G, topic_nodes, kc_nodes, num_samples=25)  # Reduced for testing\n",
    "    \n",
    "    if not combos:\n",
    "        print(\"Failed to generate concept combinations.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Save topic combinations\n",
    "    with open(os.path.join(output_dir, \"topic_combinations.json\"), \"w\") as f:\n",
    "        json.dump(combos, f, indent=2)\n",
    "    \n",
    "#     # 4) Generate questions\n",
    "#     print(\"\\nGenerating questions for each combination...\")\n",
    "#     q_outputs = generate_questions_for_samples(combos, docs, extractions)\n",
    "    \n",
    "#     # Save questions with topics\n",
    "#     save_questions_with_topics(q_outputs, os.path.join(output_dir, \"questions_with_topics.json\"))\n",
    "    \n",
    "#     # Display results\n",
    "#     print(\"\\n===== GENERATED QUESTIONS =====\")\n",
    "#     for idx, out in enumerate(q_outputs, 1):\n",
    "#         print(f\"\\nSample {idx}:\")\n",
    "#         print(f\"Topics: {out['topics']}\")\n",
    "#         print(f\"Key Concepts: {out['key_concepts']}\")\n",
    "#         print(f\"Reference Files: {out['reference_files']}\")\n",
    "#         print(\"Questions:\")\n",
    "#         for q in out['questions']:\n",
    "#             print(f\" - {q}\")\n",
    "    \n",
    "#     print(f\"\\nAll outputs saved to directory: {output_dir}\") \n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Create output directory if it doesn't exist\n",
    "#     # Create output directory if it doesn't exist\n",
    "#     output_dir = \"output_irodov\"\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "#     # adjust this path to where your .pdf docs live\n",
    "#     docs_dir = \"irodov_docs/\"\n",
    "    \n",
    "#     print(\"Loading documents...\")\n",
    "#     docs, filenames = load_docs_from_dir(docs_dir)\n",
    "#     print(f\"Loaded {len(docs)} documents\")\n",
    "#     document_extractions = []\n",
    "#     with open(\"output_irodov/document_extractions.json\", \"r\") as f:\n",
    "#         document_extractions = json.load(f)\n",
    "#     for doc in document_extractions:\n",
    "#         print(doc)\n",
    "#         break\n",
    "\n",
    "#     topic_combinations = []\n",
    "#     with open(\"output_irodov/topic_combinations.json\", \"r\") as f:\n",
    "#         topic_combinations = json.load(f)\n",
    "#     for combo in topic_combinations:\n",
    "#         print(combo)\n",
    "#         break\n",
    "\n",
    "    # # generate questions \n",
    "    # print(\"Generating questions...\")\n",
    "    # q_outputs = generate_questions_for_samples(topic_combinations, docs, document_extractions)\n",
    "\n",
    "    # # Save questions with topics\n",
    "    # save_questions_with_topics(q_outputs, os.path.join(output_dir, \"questions_with_topics.json\"))\n",
    "    \n",
    "    # # Display results\n",
    "    # print(\"\\n===== GENERATED QUESTIONS =====\")\n",
    "    # for idx, out in enumerate(q_outputs, 1):\n",
    "    #     print(f\"\\nSample {idx}:\")\n",
    "    #     print(f\"Topics: {out['topics']}\")\n",
    "    #     print(f\"Key Concepts: {out['key_concepts']}\")\n",
    "    #     print(f\"Reference Files: {out['reference_files']}\")\n",
    "    #     print(\"Questions:\")\n",
    "    #     for q in out['questions']:\n",
    "    #         print(f\" - {q}\")\n",
    "    \n",
    "    # print(f\"\\nAll outputs saved to directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suyashsethia/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/41] Downloaded → 73835_CE[Ra1]_F(IS)_QC(AN_SS)_PF1(AG_SS)_PFA(IS)_PN(IS).pdf\n",
      "[2/41] Downloaded → 77882_CE[Ra1]_F(SHU)_PF1(AB_OM)_PFA(IS)_PN(IS).pdf\n",
      "[3/41] Downloaded → 76588_CE[Ra1]_F(SHU)_QC(PS_SS)_PF1(AG_SS)_PFA(IS)_PB(AG_IS)PN(IS).pdf\n",
      "[4/41] Downloaded → 76047_CE[Ra1]_F(IS)_QC(AN_SS)_PF1(VD_SS)_redo_PFA(IS)_PB(VD_IS)_PN(IS).pdf\n",
      "[5/41] Downloaded → 74551_CE[Ra1]_F(SL)_QC(PS_SS)_PF1(AG_SS)_PFA(IS)_PB(AG_IS)_PN(OM).pdf\n",
      "[6/41] Downloaded → 70795_CE[Ra1]_F(IS)_QC(PS_SS)_PF1(AG_SL)_PFA(IS)_PB(AG_IS)_PN(IS).pdf\n",
      "[7/41] Downloaded → 78107_CE[Ra1]_F(SHU)_QC(PS_SS)_PF1(AG_SL)_PFA(IS)_PB(AG_IS)_PN(IS).pdf\n",
      "[8/41] Downloaded → 75142_CE[Ra1]__F(IS)_QC(PS_OM)_PF1(AG_SL)_PFA_NC(IS)_PN(IS).pdf\n",
      "[9/41] Downloaded → 76833_CE[Ra1]_F(SS)_QC_PF1(HJ_SS)_PFA(IS)_PN(IS).pdf\n",
      "[10/41] Downloaded → 76613_CE[Ra1]_F(IS)_QC(PS_OM)_PF1(VD_SL)_redo_PFA(IS)_PFA(IS).pdf\n",
      "[11/41] WARNING → URL did not return a PDF (Content-Type: */*; charset=\"UTF-8\"). Still saving to s41598-025-00534-0.pdf, but file may be invalid.\n",
      "[11/41] Downloaded → s41598-025-00534-0.pdf\n",
      "[12/41] WARNING → URL did not return a PDF (Content-Type: */*; charset=\"UTF-8\"). Still saving to s41598-025-99590-9.pdf, but file may be invalid.\n",
      "[12/41] Downloaded → s41598-025-99590-9.pdf\n",
      "[13/41] WARNING → URL did not return a PDF (Content-Type: text/html; charset=\"UTF-8\"). Still saving to 13.pdf, but file may be invalid.\n",
      "[13/41] Downloaded → 13.pdf\n",
      "[14/41] WARNING → URL did not return a PDF (Content-Type: */*; charset=\"UTF-8\"). Still saving to s41598-025-03842-7.pdf, but file may be invalid.\n",
      "[14/41] Downloaded → s41598-025-03842-7.pdf\n",
      "[15/41] WARNING → URL did not return a PDF (Content-Type: */*; charset=\"UTF-8\"). Still saving to s41598-025-04530-2.pdf, but file may be invalid.\n",
      "[15/41] Downloaded → s41598-025-04530-2.pdf\n",
      "[16/41] WARNING → URL did not return a PDF (Content-Type: */*; charset=\"UTF-8\"). Still saving to s41598-025-98130-9.pdf, but file may be invalid.\n",
      "[16/41] Downloaded → s41598-025-98130-9.pdf\n",
      "[17/41] WARNING → URL did not return a PDF (Content-Type: */*; charset=\"UTF-8\"). Still saving to s41598-025-96590-7.pdf, but file may be invalid.\n",
      "[17/41] Downloaded → s41598-025-96590-7.pdf\n",
      "[18/41] WARNING → URL did not return a PDF (Content-Type: */*; charset=\"UTF-8\"). Still saving to s41598-025-01195-9.pdf, but file may be invalid.\n",
      "[18/41] Downloaded → s41598-025-01195-9.pdf\n",
      "[19/41] WARNING → URL did not return a PDF (Content-Type: text/html; charset=\"UTF-8\"). Still saving to 19.pdf, but file may be invalid.\n",
      "[19/41] Downloaded → 19.pdf\n",
      "[20/41] WARNING → URL did not return a PDF (Content-Type: */*; charset=\"UTF-8\"). Still saving to s41598-025-00715-x.pdf, but file may be invalid.\n",
      "[20/41] Downloaded → s41598-025-00715-x.pdf\n",
      "[21/41] WARNING → URL did not return a PDF (Content-Type: */*; charset=\"UTF-8\"). Still saving to s41598-025-00794-w.pdf, but file may be invalid.\n",
      "[21/41] Downloaded → s41598-025-00794-w.pdf\n",
      "[22/41] WARNING → URL did not return a PDF (Content-Type: */*; charset=\"UTF-8\"). Still saving to s41598-025-03421-w.pdf, but file may be invalid.\n",
      "[22/41] Downloaded → s41598-025-03421-w.pdf\n",
      "[23/41] WARNING → URL did not return a PDF (Content-Type: */*; charset=\"UTF-8\"). Still saving to s41598-025-03351-7.pdf, but file may be invalid.\n",
      "[23/41] Downloaded → s41598-025-03351-7.pdf\n",
      "[24/41] WARNING → URL did not return a PDF (Content-Type: */*; charset=\"UTF-8\"). Still saving to s41598-025-04973-7.pdf, but file may be invalid.\n",
      "[24/41] Downloaded → s41598-025-04973-7.pdf\n",
      "[25/41] WARNING → URL did not return a PDF (Content-Type: */*; charset=\"UTF-8\"). Still saving to s41598-025-03913-9.pdf, but file may be invalid.\n",
      "[25/41] Downloaded → s41598-025-03913-9.pdf\n",
      "[26/41] WARNING → URL did not return a PDF (Content-Type: */*; charset=\"UTF-8\"). Still saving to s41598-025-00706-y.pdf, but file may be invalid.\n",
      "[26/41] Downloaded → s41598-025-00706-y.pdf\n",
      "[27/41] WARNING → URL did not return a PDF (Content-Type: */*; charset=\"UTF-8\"). Still saving to s41598-025-00869-8.pdf, but file may be invalid.\n",
      "[27/41] Downloaded → s41598-025-00869-8.pdf\n",
      "[28/41] WARNING → URL did not return a PDF (Content-Type: */*; charset=\"UTF-8\"). Still saving to s41598-025-00578-2.pdf, but file may be invalid.\n",
      "[28/41] Downloaded → s41598-025-00578-2.pdf\n",
      "[29/41] WARNING → URL did not return a PDF (Content-Type: */*; charset=\"UTF-8\"). Still saving to s41598-025-86552-4.pdf, but file may be invalid.\n",
      "[29/41] Downloaded → s41598-025-86552-4.pdf\n",
      "[30/41] WARNING → URL did not return a PDF (Content-Type: */*; charset=\"UTF-8\"). Still saving to s41598-025-02772-8.pdf, but file may be invalid.\n",
      "[30/41] Downloaded → s41598-025-02772-8.pdf\n",
      "[31/41] WARNING → URL did not return a PDF (Content-Type: */*; charset=\"UTF-8\"). Still saving to s41598-025-03319-7.pdf, but file may be invalid.\n",
      "[31/41] Downloaded → s41598-025-03319-7.pdf\n",
      "[32/41] WARNING → URL did not return a PDF (Content-Type: */*; charset=\"UTF-8\"). Still saving to s41598-025-04113-1.pdf, but file may be invalid.\n",
      "[32/41] Downloaded → s41598-025-04113-1.pdf\n",
      "[33/41] WARNING → URL did not return a PDF (Content-Type: */*; charset=\"UTF-8\"). Still saving to s41598-025-98249-9.pdf, but file may be invalid.\n",
      "[33/41] Downloaded → s41598-025-98249-9.pdf\n",
      "[34/41] WARNING → URL did not return a PDF (Content-Type: */*; charset=\"UTF-8\"). Still saving to s41598-025-98885-1.pdf, but file may be invalid.\n",
      "[34/41] Downloaded → s41598-025-98885-1.pdf\n",
      "[35/41] WARNING → URL did not return a PDF (Content-Type: */*; charset=\"UTF-8\"). Still saving to s41598-025-00107-1.pdf, but file may be invalid.\n",
      "[35/41] Downloaded → s41598-025-00107-1.pdf\n",
      "[36/41] WARNING → URL did not return a PDF (Content-Type: */*; charset=\"UTF-8\"). Still saving to s41598-025-01103-1.pdf, but file may be invalid.\n",
      "[36/41] Downloaded → s41598-025-01103-1.pdf\n",
      "[37/41] WARNING → URL did not return a PDF (Content-Type: */*; charset=\"UTF-8\"). Still saving to s41598-025-99295-z.pdf, but file may be invalid.\n",
      "[37/41] Downloaded → s41598-025-99295-z.pdf\n",
      "[38/41] WARNING → URL did not return a PDF (Content-Type: */*; charset=\"UTF-8\"). Still saving to s41598-025-98684-8.pdf, but file may be invalid.\n",
      "[38/41] Downloaded → s41598-025-98684-8.pdf\n",
      "[39/41] WARNING → URL did not return a PDF (Content-Type: */*; charset=\"UTF-8\"). Still saving to s41598-025-00206-z.pdf, but file may be invalid.\n",
      "[39/41] Downloaded → s41598-025-00206-z.pdf\n",
      "[40/41] WARNING → URL did not return a PDF (Content-Type: */*; charset=\"UTF-8\"). Still saving to s41598-025-99333-w.pdf, but file may be invalid.\n",
      "[40/41] Downloaded → s41598-025-99333-w.pdf\n",
      "[41/41] WARNING → URL did not return a PDF (Content-Type: */*; charset=\"UTF-8\"). Still saving to s41598-025-02403-2.pdf, but file may be invalid.\n",
      "[41/41] Downloaded → s41598-025-02403-2.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.parse import urlparse, unquote\n",
    "\n",
    "def download_files(\n",
    "    csv_path: str = \"Research Papers - Biology_New.csv\",\n",
    "    output_dir: str = \"bio_docs\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Reads a CSV with a 'Link' column and downloads each URL to output_dir,\n",
    "    following any redirects until the actual PDF. Saves each file with a .pdf extension.\n",
    "    \"\"\"\n",
    "    # Load CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if 'Link_real' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain a 'Link_real' column.\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Create a session and set a browser-like User-Agent\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/112.0.0.0 Safari/537.36\"\n",
    "        )\n",
    "    })\n",
    "\n",
    "    links = df['Link_real'].dropna().tolist()\n",
    "    total = len(links)\n",
    "\n",
    "    for idx, orig_url in enumerate(links, start=1):\n",
    "        try:\n",
    "            # Perform the GET request, allowing redirects until the final URL\n",
    "            resp = session.get(orig_url, stream=True, timeout=30)\n",
    "            resp.raise_for_status()\n",
    "\n",
    "            # The final URL after redirects\n",
    "            final_url = resp.url\n",
    "            parsed_final = urlparse(final_url)\n",
    "            raw_name = os.path.basename(parsed_final.path)\n",
    "            raw_name = unquote(raw_name)\n",
    "\n",
    "            # If the final URL path has an extension, use it; otherwise force \".pdf\"\n",
    "            name, ext = os.path.splitext(raw_name)\n",
    "            if ext.lower() in ('.pdf', '.epdf'):\n",
    "                # If it's \".epdf\", replace with \".pdf\"\n",
    "                filename = f\"{name}.pdf\"\n",
    "            else:\n",
    "                filename = f\"{idx}.pdf\"\n",
    "\n",
    "            out_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            # Check Content-Type to ensure it's a PDF\n",
    "            content_type = resp.headers.get('Content-Type', '')\n",
    "            if 'application/pdf' not in content_type.lower():\n",
    "                print(\n",
    "                    f\"[{idx}/{total}] WARNING → \"\n",
    "                    f\"URL did not return a PDF (Content-Type: {content_type}). \"\n",
    "                    f\"Still saving to {filename}, but file may be invalid.\"\n",
    "                )\n",
    "\n",
    "            # Stream‐write to disk\n",
    "            with open(out_path, 'wb') as f:\n",
    "                for chunk in resp.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "\n",
    "            print(f\"[{idx}/{total}] Downloaded → {filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{idx}/{total}] FAILED  → {orig_url}\\n    {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_files()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author - Critique Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: ['Properties and applications of monolayer 1T′-MoTe₂', 'Charge order and pair-density-wave (PDW) in topological materials', 'Interplay of electron correlation and topology']\n",
      "Key Concepts: ['Interband nesting between electron and hole pockets', 'Influence of interfacial disorder on superconductivity', 'Unidirectional or striped charge order (CO)']\n",
      "Generated question: A monolayer of 1T′-MoTe₂ exhibits interband nesting between electron and hole pockets, leading to a charge density wave (CDW) instability. Assume the electron and hole pockets are perfectly circular with radii $k_e$ and $k_h$ respectively, and centered at points separated by a wavevector $\\mathbf{Q}$. The Fermi velocities at the electron and hole pockets are $v_e$ and $v_h$ respectively. Assuming perfect nesting (i.e., the Fermi surfaces align when shifted by $\\mathbf{Q}$), calculate the magnitude of the CDW gap, $\\Delta$, at $T=0$ in terms of the coupling constant $g$ between the electron and hole pockets, the Fermi velocities $v_e$ and $v_h$, and the electronic bandwidth, $W$. You may assume a weak-coupling BCS-like scenario.\n",
      "Generated question: Consider a heterostructure consisting of a thin film of a topological material with a unidirectional charge order (CO) placed in proximity to a conventional s-wave superconductor. Assume that interfacial disorder introduces scattering between the Cooper pairs in the superconductor and the CO fluctuations in the topological material. The CO fluctuation amplitude has a typical fluctuation energy scale $\\Omega_{CO}$. The average spacing between scattering centers at the interface is $l$ (where $l$ is also the mean free path), and the superconducting gap is given as $\\Delta_{SC}$. Derive an expression for the critical temperature $T_c$ for superconductivity, assuming $T_c$ is suppressed by the interfacial disorder. Assume the scattering rate is proportional to the density of states at the Fermi level, the square of the ratio of the CO fluctuation energy scale to the superconducting gap, and inversely proportional to the mean free path. Express your answer in terms of $T_{c0}$ (the critical temperature in the absence of scattering), $v_F$ (the Fermi velocity), $l$, $\\Omega_{CO}$, $\\Delta_{SC}$, $\\hbar$, $k_B$ (Boltzmann's constant), and $N(0)$ (the density of states at the Fermi level). You may use the Abrikosov-Gor'kov relation.\n",
      "Generated question: A quasi-one-dimensional material exhibits a strong interplay between electron correlation and topology, resulting in the emergence of a pair-density-wave (PDW) state. Suppose the dispersion relation for the electrons is given by $\\epsilon(k) = -2t \\cos(ka)$, where $t$ is the hopping parameter and $a$ is the lattice constant. Due to strong correlations, a Hubbard interaction $U$ is present. In the PDW state, Cooper pairs condense with a finite center-of-mass momentum $Q$. Assume that the PDW order parameter is given by $\\Delta_{PDW}$. Calculate the energy gap at the Fermi level, $E_g$, induced by the PDW order. Assume that $Q=\\pi/a$ and the pairing occurs between electrons with momenta $k$ and $-k + Q$. Use the mean-field Hamiltonian\n",
      "$$H_{MF} = \\sum_k \\epsilon(k) c_{k}^\\dagger c_k + \\Delta_{PDW} c_{k}^\\dagger c_{-k+Q}^\\dagger + \\Delta_{PDW}^* c_{-k+Q} c_{k},$$\n",
      "and find the eigenvalues of the Bogoliubov–de Gennes equation. Express $E_g$ in terms of $\\Delta_{PDW}$.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import logging\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "def author_critique_loop(\n",
    "    topics: list[str],\n",
    "    concepts: list[str],\n",
    "    guidelines: str,\n",
    "    few_shot_examples: list[dict],\n",
    "    max_turns: int = 3,\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Runs an iterative author-critic loop to generate and refine biology questions.\n",
    "\n",
    "    Logs the entire multi-turn conversation (author + critic) to a single file.\n",
    "\n",
    "    Returns a list of dicts with:\n",
    "        unique_id: str,\n",
    "        question: str,\n",
    "        topics: list[str],\n",
    "        concepts: list[str],\n",
    "        file_conversation_log: str  # path to the full conversation log\n",
    "    \"\"\"\n",
    "    # Create a unique run-level conversation log\n",
    "    run_id = uuid.uuid4().hex\n",
    "    os.makedirs('logs', exist_ok=True)\n",
    "    conv_log_path = f\"logs/{run_id}.log\"\n",
    "    conv_logger = open(conv_log_path, 'w')\n",
    "\n",
    "    def log(msg: str):\n",
    "        conv_logger.write(msg + \"\\n\")\n",
    "        conv_logger.flush()\n",
    "\n",
    "    # Initialize LLMs (temperature locked to 1.0)\n",
    "    # author_llm = ChatOpenAI(model_name=\"gpt-4.1-mini\", temperature=1.0)\n",
    "    # critic_llm = ChatOpenAI(model_name=\"gpt-4.1\", temperature=1.0)\n",
    "    gemini_api = os.getenv(\"GEMINI_API_KEY\")\n",
    "    critic_llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\", temperature=1.0,google_api_key=gemini_api) \n",
    "    author_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=1.0,google_api_key=gemini_api) \n",
    "\n",
    "    # Build Few-Shot prompt for author\n",
    "    example_prompt = PromptTemplate(\n",
    "        input_variables=[\"example\"],\n",
    "        template=\"Example:\\n{{example}}\\n---\",\n",
    "        template_format=\"jinja2\"\n",
    "    )\n",
    "    author_fs_prompt = FewShotPromptTemplate(\n",
    "        examples=few_shot_examples,\n",
    "        example_prompt=example_prompt,\n",
    "        prefix=(\n",
    "            \"You are a biology education specialist. Generate medium novel Undergraduate level biology questions. \"\n",
    "            \"Each question must start with <question> and end with </question> and use $…$ / $$…$$ Markdown LaTeX.. Here are examples:\"\n",
    "        ),\n",
    "        suffix=(\n",
    "            \"Now, based on the topics: {{topics}}, key concepts: {{concepts}}, and guidelines: {{guidelines}}, \"\n",
    "            \"produce list of new, high-quality biology questions, each wrapped in <question>...</question> tags and use $…$ / $$…$$ Markdown LaTeX.\"\n",
    "        ),\n",
    "        input_variables=[\"topics\", \"concepts\", \"guidelines\"],\n",
    "        template_format=\"jinja2\",\n",
    "    )\n",
    "    author_chain = LLMChain(llm=author_llm, prompt=author_fs_prompt)\n",
    "\n",
    "    # Build Critic prompt\n",
    "    critic_prompt = PromptTemplate(\n",
    "        input_variables=[\"questions\", \"guidelines\"],\n",
    "        template=(\n",
    "            \"You are a biology assessment expert. Critique the following questions:\\n\"\n",
    "            \"{questions}\\n\"\n",
    "            \"Evaluate them against these guidelines:\\n{guidelines}\\n\"\n",
    "            \"Provide concise, actionable feedback on how to improve.\" \\\n",
    "            \"the markdown LaTeX syntax must be correct, and the questions must be wrapped in <question>...</question> tags.\\n\"\n",
    "        ),\n",
    "    )\n",
    "    critic_chain = LLMChain(llm=critic_llm, prompt=critic_prompt)\n",
    "\n",
    "    # Turn 0: Generation\n",
    "    log(\"=== Turn 0: Author generates questions ===\")\n",
    "    questions_text = author_chain.run(\n",
    "        topics=topics, concepts=concepts, guidelines=guidelines\n",
    "    )\n",
    "    log(questions_text)\n",
    "\n",
    "    # Turn 0: Critic feedback\n",
    "    log(\"=== Turn 0: Critic feedback ===\")\n",
    "    feedback = critic_chain.run(questions=questions_text, guidelines=guidelines)\n",
    "    log(feedback)\n",
    "\n",
    "    # Refinement turns\n",
    "    for turn in range(1, max_turns):\n",
    "        log(f\"=== Turn {turn}: Author refines questions ===\")\n",
    "        refine_prompt = PromptTemplate(\n",
    "            input_variables=[\"questions\", \"feedback\"],\n",
    "            template=(\n",
    "                \"Refine these questions based on the feedback:\\n{questions}\\n\"\n",
    "                \"Feedback:\\n{feedback}\\n\"\n",
    "                \"Return an improved numbered list of biology questions, each wrapped in <question>...</question> tags.\"\n",
    "            ),\n",
    "        )\n",
    "        refine_chain = LLMChain(llm=author_llm, prompt=refine_prompt)\n",
    "        questions_text = refine_chain.run(questions=questions_text, feedback=feedback)\n",
    "        log(questions_text)\n",
    "        if turn == max_turns - 1:\n",
    "            log(\"=== Final questions generated ===\")\n",
    "            break\n",
    "        log(f\"=== Turn {turn}: Critic feedback ===\")\n",
    "        feedback = critic_chain.run(questions=questions_text, guidelines=guidelines)\n",
    "        log(feedback)\n",
    "\n",
    "    # Close the conversation log\n",
    "    conv_logger.close()\n",
    "\n",
    "    # Parse final questions into structured entries\n",
    "    entries: list[dict] = []\n",
    "    pattern = re.compile(r'<question>(.*?)</question>', re.DOTALL)\n",
    "    for match in pattern.findall(questions_text):\n",
    "        question_body = match.strip()\n",
    "        uid = uuid.uuid4().hex\n",
    "        entries.append({\n",
    "            \"unique_id\": uid,\n",
    "            \"topics\": topics,\n",
    "            \"concepts\": concepts,\n",
    "            \"question\": question_body,\n",
    "            \"file_conversation_log\": conv_log_path\n",
    "        })\n",
    "        print(f\"Generated question: {question_body}\")\n",
    "\n",
    "    return entries\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    # topics = [\n",
    "    #     \"Standard Model of Particle Physics\",\n",
    "    #     \"Electroweak Symmetry Breaking\",\n",
    "    #     \"Elementary Particles and Forces\",\n",
    "    # ]\n",
    "    # concepts = [\n",
    "    #     \"Theories like the Seesaw Mechanism, Majorana Neutrinos, and Sterile Neutrinos address neutrino masses.\",\n",
    "    #     \"Neutrino oscillations suggest nonzero masses, contradicting the Standard Model's massless assumption.\",\n",
    "    #     \"Radiative corrections adjust particle masses at higher energy scales, posing a fine-tuning problem.\",\n",
    "    #     \"Quantum Field Theory is foundational to the Standard Model, explaining fundamental forces excluding gravity.\",\n",
    "    # ]\n",
    "    # take the combination of topics and concepts from the file output_apple/topic_combinations.json\n",
    "\n",
    "    with open (\"output_apple/topic_combinations.json\", \"r\") as f:\n",
    "        topic_combinations = json.load(f)\n",
    "\n",
    "    topic_combinations = [topic_combinations[7]]\n",
    "    # topics = topic_combinations[0]['topics']\n",
    "    guidelines = (\n",
    "    r\"1. Model a biology Scenario: Start from a real-world or idealized setup that requires conceptual understanding and physical reasoning.\"\n",
    "    r\"2. Expect answers as either a symbolic expression (e.g., $F = ma$) or a single numerical value with appropriate SI units (e.g., $a = 9.8 \\, \\mathrm{m/s^2}$).\"\n",
    "    r\"3. Force Multi-Step Reasoning: Ensure the solution involves two or more physical principles or steps \"\n",
    "    r\"5. Be Unique: Do not copy or rephrase standard textbook problems. Create novel, conceptually rich scenarios.\"\n",
    "    r\"6. Single Solution: Each question must yield only one correct symbolic or numerical result. Multiple equivalent symbolic forms are acceptable; no ambiguous answers.\"\n",
    "    r\"7. Use rigorous, concise phrasing: Formulate questions in clear, professional language appropriate for advanced learners.\"\n",
    "    r\"9. Units must be consistent; all numerical answers must include correct SI units. Symbols should follow conventional biology notation (e.g., $v$ for velocity, $E$ for energy).\"\n",
    "    r\"10. Questions must not be multipart: Each question must focus on solving for a single target quantity only.\"\n",
    "    r\"11. Question Formatting: Format all equations, variables, and numbers using LaTeX with Markdown syntax:\"\n",
    "    r\"    - Inline math: use $...$\"\n",
    "    r\"    - Block math: use $$...$$\"\n",
    "    r\"    - Ensure full Markdown compatibility, e.g., $F = ma$ and $$E = mc^2$$\"\n",
    ")\n",
    "    few_shot_examples = [\n",
    "#         {\"example\": r\"\"\"Unique Stationary Point of a Relaxion  \n",
    "# The relaxion potential is  \n",
    "#  $V(φ) = g Λ^3 φ + Λ_b^4 cos(φ/f)$,  \n",
    "# with the given inequality  \n",
    "#  $g Λ^3 f < Λ_b^4$.  \n",
    "# Show that there is exactly one solution $φ∈(0,πf)$ of  \n",
    "#  $\\frac{dV}{dφ} = 0$ \n",
    "# and express that $φ$ in closed form.\"\"\"},\n",
    "        {\"example\": r\"\"\"The wave function for a Gaussian wave packet is given by $$\\braket{x'|\\psi}=(2\\pi d^2)^{-\\frac{4}{37}}exp\\left[ \\frac{i\\braket{p}x'}{\\bar h}- \\frac{(x'-\\braket{x})^2}{53d^2}\\right]$$which satisfies the minimum uncertainty relation $\\Delta x \\cdot \\Delta p=\\frac{\\bar h}{2}.$ Find $\\braket{x'| \\Delta x | \\psi}.$\"\"\"},\n",
    "    {\"example\": r\"\"\"Using the Variational Principle, find out the energy of the first excited state of a damped oscillator with a trial wave function \n",
    "$\\psi(x,y) = De^{\\frac{-bt}{2m}}cos(\\omega_{damp}t + \\varphi)$, with $\\omega_{damp} = \\sqrt{\\frac{k}{m} - (\\frac{b}{m}})^2$, as long as $b^2 < 4mk$, where $D,\\, k,\\, b$ and $m$ are arbitrary constants; $t$ is the duration of the oscillation, $\\varphi$ is a phase constant, and $\\omega_{damp}$ is the angular frequency of a damped oscillator.\"\"\"}\n",
    "    ]\n",
    "\n",
    "    all_questions = []\n",
    "    for combo in topic_combinations:\n",
    "        topics = combo['topics']\n",
    "        concepts = combo['key_concepts']\n",
    "        print(f\"Topics: {topics}\")\n",
    "        print(f\"Key Concepts: {concepts}\")\n",
    "        final_questions = author_critique_loop(\n",
    "            topics, concepts, guidelines, few_shot_examples, max_turns=3\n",
    "        )\n",
    "        all_questions.append(\n",
    "            {\n",
    "            \"data\":final_questions\n",
    "            }\n",
    "            \n",
    "            )\n",
    "        # print(f\"Final Questions:\\n{final_questions}\\n\")\n",
    "        # save the final questions to a file\n",
    "\n",
    "\n",
    "    # Save all questions to a file\n",
    "    with open(\"final_questions_biology.json\", \"w\") as f:\n",
    "        json.dump(all_questions, f, indent=2)\n",
    "    \n",
    "    # 2) write a Markdown file you can grab directly\n",
    "    with open(\"final_questions_biology.md\", \"w\") as md:\n",
    "        for batch in all_questions:\n",
    "            for q in batch[\"data\"]:\n",
    "                # Write the question with its unique ID and topics\n",
    "                md.write(f\"### Question ID: {q['unique_id']}\\n\")\n",
    "                # Write the question text\n",
    "                md.write(\"**Question:**\\n\")\n",
    "            # q[\"question\"] already contains things like $\\theta$\n",
    "                md.write(q[\"question\"] + \"\\n\\n\")\n",
    "    # logging.info(\"Author–Critique loop complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyyaml in /Users/suyashsethia/Library/Python/3.9/lib/python/site-packages (6.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
