{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 documents\n",
      "{'filename': '2505.00694v1.pdf', 'topics': ['Hierarchy Problem', 'Electroweak Symmetry Breaking (EWSB)', 'Standard Model and Higgs Boson', 'Supersymmetry', 'Quantum Field Theory', 'High Energy Physics', 'Future Colliders', 'Naturalness in Physics'], 'key_concepts': ['Hierarchy Problem: Concerns the large discrepancy between the weak force scale and the Planck scale, needing an explanation beyond the Standard Model.', 'Higgs Mechanism: The role and mass of the Higgs boson in electroweak symmetry breaking within the Standard Model.', \"Wilsonian Effective Field Theory: A viewpoint addressing the significance of energy scales in quantum field theories inspired by Ken Wilson's work.\", 'Naturalness: The principle that addresses the fine-tuning of parameters in physics, especially concerning particle masses.', 'Supersymmetry (SUSY): A theoretical framework that extends the Standard Model, solving some hierarchy problems by predicting partners for every Standard Model particle.', 'Little Hierarchy Problem: The puzzle of why new particles related to EWSB have not yet been observed at the Large Hadron Collider (LHC).', 'Role of High Energy Colliders: Future colliders, like a 100 TeV proton collider, are posited as necessary to uncover new physics that current models predict.', 'Post-Naturalness Era: A phase in theoretical physics characterized by exploring beyond traditional solutions to the Hierarchy Problem, such as anthropic principles or cosmological considerations.']}\n",
      "{'topics': ['Electroweak Hierarchy Problem', 'Cosmological Constant Problem', 'UV/IR Mixing'], 'key_concepts': ['Future experimental exploration in particle physics', 'Experimental tests for naturalness', 'Supersymmetric solutions to naturalness problems', 'Wilsonian Naturalness', 'Anthropic Principle']}\n",
      "Generating questions...\n",
      "Saved questions with topic combinations to output/questions_with_topics.json\n",
      "\n",
      "===== GENERATED QUESTIONS =====\n",
      "\n",
      "Sample 1:\n",
      "Topics: ['Electroweak Hierarchy Problem', 'Cosmological Constant Problem', 'UV/IR Mixing']\n",
      "Key Concepts: ['Future experimental exploration in particle physics', 'Experimental tests for naturalness', 'Supersymmetric solutions to naturalness problems', 'Wilsonian Naturalness', 'Anthropic Principle']\n",
      "Reference Files: ['2205.05708v1.pdf', '2505.00652v1.pdf']\n",
      "Questions:\n",
      " - How does the electroweak hierarchy problem influence future experimental exploration in particle physics?\n",
      " - What role does supersymmetry play in addressing the naturalness problems associated with the electroweak hierarchy?\n",
      " - In the context of the cosmological constant problem, how might UV/IR mixing impact the development of new quantum gravity theories?\n",
      " - Discuss how future experimental tests might help to confirm or deny the anthropic principle as an explanation for the cosmological constant problem.\n",
      " - What are the key challenges facing experimental tests for naturalness in identifying solutions to the strong CP problem?\n",
      " - How does Wilsonian Naturalness contribute to our understanding of UV/IR mixing in high energy physics?\n",
      " - In what ways could exploring the landscape of vacua provide insights into the cosmological constant and electroweak hierarchy problems?\n",
      " - What experimental signatures might indicate a successful resolution of the electroweak hierarchy problem through the lens of the Anthropic Principle?\n",
      " - How do theories involving UV/IR mixing propose to solve the electroweak hierarchy problem at scales above the TeV range?\n",
      " - Explain how the application of supersymmetric solutions might mitigate tensions related to future experimental tests of particle physics naturalness.\n",
      "\n",
      "Sample 2:\n",
      "Topics: ['Quantum Field Theory', 'Standard Model and Higgs Boson', 'Supersymmetry']\n",
      "Key Concepts: ['Post-Naturalness Era: A phase in theoretical physics characterized by exploring beyond traditional solutions to the Hierarchy Problem, such as anthropic principles or cosmological considerations.', 'Higgs Mechanism: The role and mass of the Higgs boson in electroweak symmetry breaking within the Standard Model.']\n",
      "Reference Files: ['2505.00694v1.pdf', '2205.05708v1.pdf']\n",
      "Questions:\n",
      " - ```json\n",
      "[\n",
      "  \"Consider a scenario in a composite Higgs model where the Higgs boson is a bound state of new strong dynamics. If the compositeness scale is \\\\( \\Lambda_c \\\\), derive an expression for the Higgs boson mass \\\\( m_h \\\\) in terms of \\\\( \\Lambda_c \\\\) and any relevant coupling constants or parameters.\",\n",
      "  \"Imagine a supersymmetric model where gravity is described by supergravity and the electroweak scale is stabilized by a new physics mechanism at scale \\\\( M_* \\\\). Derive a relation that expresses the observed electroweak scale \\\\( v \\\\) in terms of \\\\( M_* \\\\), the Planck mass \\\\( M_{Pl} \\\\), and other relevant parameters.\",\n",
      "  \"In a theoretical universe described by a Randall-Sundrum scenario with a warped extra dimension, derive a relation for the effective four-dimensional Planck scale \\\\( M_{Pl} \\\\) in terms of the AdS curvature scale \\\\( k \\\\), the five-dimensional Planck scale \\\\( M_5 \\\\), and any warping parameters.\",\n",
      "  \"Given a model with a fermionic dark matter candidate within a neutral naturalness framework such as the Twin Higgs, derive an expression for the relic density of dark matter \\\\( \\Omega_{DM} \\\\) in terms of the dark matter mass \\\\( m_{DM} \\\\) and any relevant cross-sections or temperatures.\",\n",
      "  \"In a cosmological relaxation of the electroweak scale scenario involving a 'relaxion' field, derive an expression for the stabilization point of this field \\\\( \\phi \\\\) in terms of the QCD confinement scale \\\\( \\Lambda_{QCD} \\\\), the relaxion coupling constants, and the electroweak scale \\\\( v \\\\).\"\n",
      "]\n",
      "```\n",
      "\n",
      "Sample 3:\n",
      "Topics: ['Electroweak Hierarchy Problem', 'UV/IR Mixing']\n",
      "Key Concepts: ['Hierarchy problems beyond particle physics', 'Landscape of vacua', 'Anthropic Principle', 'Implications of the Higgs mass']\n",
      "Reference Files: ['2205.05708v1.pdf', '2505.00652v1.pdf']\n",
      "Questions:\n",
      " - Consider a universe where the Standard Model exists in a landscape of vacua. Describe the role that the anthropic principle might play in explaining the smallness of the cosmological constant. Provide your answer as an expression in LaTeX.\n",
      " - A hypothetical particle physics scenario introduces a pseudoscalar boson that couples to gluons. The vacuum energy density associated with this boson is compensated by another hidden sector field through a symmetry-breaking potential. Derive an expression for the minimum energy configuration of the system in LaTeX.\n",
      " - In a quantum field theory with UV/IR mixing, propose a mechanism based on a higher-dimensional operator that dynamically relaxes the electroweak scale. Derive a symbolic expression for the resulting Higgs mass, incorporating any anomalies or symmetries that play a role.\n",
      " - Consider a scenario in cosmology where a relaxion-like field interacts with both Standard Model and hidden sector particles through a spontaneously broken global symmetry. Derive the expression for the field potential at cosmological scales in LaTeX.\n",
      " - Propose a new model of neutral naturalness involving discrete symmetries that address the electroweak hierarchy problem. Provide a symbolic expression for the mass of the Higgs boson derived from this model.\n",
      " - Discuss the implications of a UV-completed parity solution to the strong CP problem. Derive an expression for the effective CP-violating angle in terms of parity symmetry breaking scales.\n",
      " - Consider a field theory model where UV/IR mixing leads to a prediction of the electroweak hierarchy problem's solution. Formulate a symbolic expression for the cutoff scale in the model, explaining how the mixing affects the hierarchy.\n",
      " - In the context of the cosmological constant problem, assume that a new symmetry emerges in the deep infrared regime. Derive an expression that captures the effective cosmological constant in terms of this symmetry.\n",
      " - Evaluate the use of non-renormalizable operators in addressing the electroweak hierarchy problem by incorporating UV/IR mixing effects. Derive a formula for the corrections to the Higgs mass in such a theoretical framework.\n",
      "\n",
      "Sample 4:\n",
      "Topics: ['Multiplicity Fluctuations in QCD', 'Jet Substructure and Multiplicity Bias']\n",
      "Key Concepts: ['Jet hardness scale and its effect on multiplicity', 'Laplace transform techniques in solving QCD equations', 'Phenomenology of e+e− annihilation and LHC jets', 'ATLAS and CMS experimental observations in jet physics', 'Rattlesnake Effect (RSE) and its implications']\n",
      "Reference Files: ['2505.00652v1.pdf', '2205.05708v1.pdf']\n",
      "Questions:\n",
      " - {'question': 'In the context of multiplicity fluctuations in perturbative QCD, consider a gluon jet with a hardness scale given by Q. If the multiplicity distribution is described by a P-KNO function \\\\\\\\Psi(\\\\\\\\nu) with parameters from the Modified Double Log Approximation (MDLA), derive the expression for \\\\\\\\Psi(\\\\\\\\nu) using a Laplace transform approach and considering the fluctuation tail. Assume the model includes a running coupling constant and a characteristic multiplicity moment determined by the steepest descent.'}\n",
      " - {'question': \"Explore the phenomena of jet hardness scales on multiplicity distributions as observed in ATLAS jets. Given a quark jet with an energy scale Q and angular radius R, calculate the expected tail behavior of the multiplicity distribution \\\\\\\\Psi(\\\\\\\\nu) using effective degrees of freedom and include relevant parameters due to energy conservation constraints. Consider the Rattlesnake Effect's impact on high multiplicity tails.\"}\n",
      " - {'question': 'Discuss the role of anomalous dimensions in the context of QCD multiplicity fluctuations. Derive the expression for the gluon P-KNO tail, starting from the differential form of the factorial moments. Emphasize the influence of next-to-leading corrections and the significance of the MDLA in determining the proper scaling regime. Assume a consistent structure in the hard process interactions.'}\n",
      " - {'question': 'Analyze the impact of parity models on solving the strong CP problem. Starting with a basic parity-invariant sector at high energy, derive how the spontaneous symmetry breaking leads to a minimal correction to the CP-violating parameter \\\\\\\\theta in QCD. Include calculations referencing the likelihood of observable changes in neutron electric dipole moments at low energy due to parity protection.'}\n",
      " - {'question': 'For a given cosmological constant problem, consider the Abbott model for relaxation. Derive how the evolving vacuum energy density during an inflationary epoch leads to a small, positive cosmological constant. Include expressions for the evolution of spatial volume energy contributions over eons and the role of a confinement scale. Discuss how these imply a long-term in-universe observable impact.'}\n",
      " - {'question': \"In the context of hierarchical mixing, explore how weak gravity conjecture (WGC) could naturally predict the electroweak scale. Using a U(1) extension in a standard model with neutrino masses, derive constraints on \\\\\\\\lambda as a function of U(1)'s charge-to-mass ratio, ensuring that the weak scale remains protected up to typical cutoff assumptions. Examine how these insights reflect on the hierarchy problem.\"}\n",
      "\n",
      "Sample 5:\n",
      "Topics: ['Quantum Field Theory', 'Standard Model and Higgs Boson', 'Supersymmetry']\n",
      "Key Concepts: ['Post-Naturalness Era: A phase in theoretical physics characterized by exploring beyond traditional solutions to the Hierarchy Problem, such as anthropic principles or cosmological considerations.', 'Role of High Energy Colliders: Future colliders, like a 100 TeV proton collider, are posited as necessary to uncover new physics that current models predict.', \"Wilsonian Effective Field Theory: A viewpoint addressing the significance of energy scales in quantum field theories inspired by Ken Wilson's work.\", 'Supersymmetry (SUSY): A theoretical framework that extends the Standard Model, solving some hierarchy problems by predicting partners for every Standard Model particle.']\n",
      "Reference Files: ['2505.00694v1.pdf', '2205.05708v1.pdf']\n",
      "Questions:\n",
      " - {'question': 'Consider a 100 TeV proton collider designed to explore physics beyond the Standard Model. Assuming the collider uncovers a new scalar field with interaction properties similar to the Higgs boson but with a mass m_S ≈ 10 TeV, derive the energy scale Λ where new physics interactions become relevant, using Wilsonian Effective Field Theory principles.', 'topic': 'Quantum Field Theory', 'key_concepts': 'Wilsonian Effective Field Theory, New Physics Energy Scale'}\n",
      " - {'question': 'In a supersymmetric (SUSY) model extending the Standard Model to solve the hierarchy problem, new particles are expected, including superpartners of quarks and leptons. If the mass of the lightest superpartner is m_{SUSY}, calculate the one-loop radiative corrections to the Higgs boson mass parameter m_H within this framework. Express your answer in terms of m_{SUSY}, the top-quark Yukawa coupling y_t, and the ultraviolet cutoff Λ.', 'topic': 'Supersymmetry', 'key_concepts': 'Radiative Corrections, Higgs Mass Parameter, Supersymmetric Extensions'}\n",
      " - {'question': 'In the context of the Post-Naturalness era, evaluate the impact of employing high-energy colliders on the discovery of solutions to the Little Hierarchy Problem. Assume a future collider capable of exploring scales up to 10 TeV and consider the contributions of both supersymmetric and non-supersymmetric particles. Derive an expression for the sensitivity of such a collider to deviations in electroweak precision observables due to new physics at this energy scale.', 'topic': 'Standard Model and Higgs Boson', 'key_concepts': 'Post-Naturalness Era, Little Hierarchy Problem, Electroweak Precision Tests'}\n",
      " - {'question': 'In a quantum field theory incorporating Wilsonian Effective Field Theory, model a scenario where the Higgs boson is part of a composite particle. Derive the expression for the cut-off scale Λ below which the compositeness of the Higgs can remain undetected in current high-energy collider experiments, given the Higgs coupling λ_H, the top-quark Yukawa coupling y_t, and a new physics scale m_*.', 'topic': 'Quantum Field Theory', 'key_concepts': 'Wilsonian Effective Field Theory, Composite Higgs Model'}\n",
      " - {'question': 'Suppose supersymmetry (SUSY) offers a solution to stabilizing the Higgs mass against quadratic divergences. Given a supersymmetric model with spontaneously broken SUSY at a scale f_S and a gravitino mass m_g, derive an expression for the effective potential V(φ) of a scalar field φ that remains light compared to heavy superpartners in the presence of high-scale SUSY breaking.', 'topic': 'Supersymmetry', 'key_concepts': 'Spontaneous Supersymmetry Breaking, Effective Potential'}\n",
      "\n",
      "All outputs saved to directory: output\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "import pdfplumber\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key from environment variable\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# -- Document Loading ------------------------------------------------------\n",
    "\n",
    "def load_docs_from_dir(dir_path):\n",
    "    \"\"\"\n",
    "    Load all .pdf files from a directory and extract their full text.\n",
    "    Returns a list of strings, one per PDF.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    filenames = []\n",
    "    for filepath in glob.glob(os.path.join(dir_path, '*.pdf')):\n",
    "        try:\n",
    "            text_pages = []\n",
    "            with pdfplumber.open(filepath) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    text_pages.append(page.extract_text() or \"\")\n",
    "            full_text = \"\\n\".join(text_pages)\n",
    "            docs.append(full_text)\n",
    "            filenames.append(os.path.basename(filepath))\n",
    "        except Exception as e:\n",
    "            # skip unreadable PDFs\n",
    "            print(f\"Warning: could not load {filepath}: {e}\")\n",
    "    return docs, filenames\n",
    "\n",
    "# -- 1. Extract topics and key concepts -----------------------------------\n",
    "\n",
    "def extract_json_from_text(text):\n",
    "    \"\"\"Helper function to extract JSON content from markdown code blocks or raw text\"\"\"\n",
    "    # Try to extract JSON from markdown code blocks\n",
    "    json_match = re.search(r'```(?:json)?\\s*(.*?)\\s*```', text, re.DOTALL)\n",
    "    if json_match:\n",
    "        return json_match.group(1).strip()\n",
    "    \n",
    "    # If no code blocks, try to find JSON-like structures\n",
    "    json_match = re.search(r'(\\{.*\\})', text, re.DOTALL)\n",
    "    if json_match:\n",
    "        return json_match.group(1).strip()\n",
    "    \n",
    "    # Return original text if no JSON structure found\n",
    "    return text\n",
    "\n",
    "def extract_concepts_from_docs(doc_texts, filenames, model=\"gpt-4o\"):  \n",
    "    \"\"\"\n",
    "    Call OpenAI API to extract topics and key concepts from each document.\n",
    "    Returns list of dicts with keys 'topics' and 'key_concepts'.\n",
    "    \"\"\"\n",
    "    extractions = []\n",
    "    for i, text in enumerate(doc_texts):\n",
    "        filename = filenames[i] if i < len(filenames) else f\"doc_{i}\"\n",
    "        \n",
    "        prompt = (\n",
    "            \"Extract high-level topics and key concepts from the following document. \"\n",
    "            f\"Return JSON with keys 'topics' and 'key_concepts'.\\n\\n{text}\"\n",
    "        )\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert summarizer. Return your response as a JSON object with 'topics' and 'key_concepts' as arrays.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        output = response.choices[0].message.content\n",
    "        print(\"Raw output:\", output)\n",
    "        \n",
    "        try:\n",
    "            # Clean and extract JSON from the output\n",
    "            json_str = extract_json_from_text(output)\n",
    "            data = json.loads(json_str)\n",
    "            \n",
    "            # Ensure we have the expected keys\n",
    "            if \"topics\" in data and \"key_concepts\" in data:\n",
    "                extraction = {\n",
    "                    \"filename\": filename,\n",
    "                    \"topics\": data[\"topics\"],\n",
    "                    \"key_concepts\": data[\"key_concepts\"]\n",
    "                }\n",
    "                extractions.append(extraction)\n",
    "                print(f\"Successfully extracted {len(data['topics'])} topics and {len(data['key_concepts'])} key concepts from {filename}\")\n",
    "            else:\n",
    "                print(f\"Warning: Parsed JSON doesn't have expected keys: {data.keys()}\")\n",
    "                extractions.append({\n",
    "                    \"filename\": filename,\n",
    "                    \"topics\": [], \n",
    "                    \"key_concepts\": []\n",
    "                })\n",
    "                \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Failed to parse JSON: {e}\")\n",
    "            extractions.append({\n",
    "                \"filename\": filename,\n",
    "                \"topics\": [], \n",
    "                \"key_concepts\": []\n",
    "            })\n",
    "    \n",
    "    return extractions\n",
    "\n",
    "# -- 2. Construct the concept graph ----------------------------------------\n",
    "\n",
    "def build_concept_graph(extractions, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Build a unified graph G where nodes are topics + key concepts and\n",
    "    edges weighted by log(freq+eps) based on co-occurrence in docs.\n",
    "    Returns: G (nx.Graph), topic_nodes, kc_nodes\n",
    "    \"\"\"\n",
    "    freq = Counter()\n",
    "    all_topics, all_kcs = set(), set()\n",
    "\n",
    "    for ex in extractions:\n",
    "        nodes = ex[\"topics\"] + ex[\"key_concepts\"]\n",
    "        for i in range(len(nodes)):\n",
    "            for j in range(i+1, len(nodes)):\n",
    "                u, v = sorted((nodes[i], nodes[j]))\n",
    "                freq[(u, v)] += 1\n",
    "        all_topics.update(ex[\"topics\"])\n",
    "        all_kcs.update(ex[\"key_concepts\"])\n",
    "\n",
    "    G = nx.Graph()\n",
    "    for (u, v), f in freq.items():\n",
    "        weight = math.log(f + eps)\n",
    "        G.add_edge(u, v, weight=weight)\n",
    "\n",
    "    print(f\"Graph built with {len(all_topics)} topics and {len(all_kcs)} key concepts\")\n",
    "    return G, all_topics, all_kcs\n",
    "\n",
    "# -- Helpers for sampling --------------------------------------------------\n",
    "\n",
    "def softmax(weights):\n",
    "    exps = [math.exp(w) for w in weights]\n",
    "    s = sum(exps) or 1.0\n",
    "    return [e/s for e in exps]\n",
    "\n",
    "\n",
    "def random_walk(G, start, steps):\n",
    "    \"\"\"\n",
    "    Random walk on graph G for given steps from 'start',\n",
    "    with transition probabilities via softmax over edge weights.\n",
    "    \"\"\"\n",
    "    path = [start]\n",
    "    current = start\n",
    "    for _ in range(steps):\n",
    "        nbrs = list(G[current])\n",
    "        if not nbrs:\n",
    "            break\n",
    "        weights = [G[current][n]['weight'] for n in nbrs]\n",
    "        probs = softmax(weights)\n",
    "        current = random.choices(nbrs, probs)[0]\n",
    "        path.append(current)\n",
    "    return path\n",
    "\n",
    "# -- 3. Concept combination sampling ---------------------------------------\n",
    "\n",
    "def sample_concept_combinations(\n",
    "    G, topic_nodes, kc_nodes,\n",
    "    num_samples=100,\n",
    "    topic_walk_steps=(1, 2),\n",
    "    kc_walk_steps=(3, 4)\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate sampled sets of topics and key concepts via multi-stage random walks.\n",
    "    Returns list of dicts: {'topics': set, 'key_concepts': set}\n",
    "    \"\"\"\n",
    "    # Safety check\n",
    "    if not topic_nodes:\n",
    "        print(\"Error: No topics found. Cannot sample combinations.\")\n",
    "        return []\n",
    "        \n",
    "    G_topic = G.subgraph(topic_nodes)\n",
    "    G_topic_kc = G.subgraph(topic_nodes | kc_nodes)\n",
    "    G_kc = G.subgraph(kc_nodes)\n",
    "    samples = []\n",
    "\n",
    "    topics_list = list(topic_nodes)\n",
    "    print(f\"Sampling from {len(topics_list)} topics\")\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        t0 = random.choice(topics_list)\n",
    "        t_steps = random.choice(topic_walk_steps)\n",
    "        topic_path = random_walk(G_topic, t0, t_steps)\n",
    "        sampled_topics = set(topic_path)\n",
    "\n",
    "        kc_cands = [nbr for t in sampled_topics for nbr in G_topic_kc[t] if nbr in kc_nodes]\n",
    "        if kc_cands:\n",
    "            k0 = random.choice(kc_cands)\n",
    "            k_steps = random.choice(kc_walk_steps)\n",
    "            kc_path = random_walk(G_kc, k0, k_steps)\n",
    "            sampled_kcs = set(kc_path)\n",
    "        else:\n",
    "            sampled_kcs = set()\n",
    "\n",
    "        samples.append({\n",
    "            \"topics\": list(sampled_topics),  # Convert sets to lists for JSON serialization\n",
    "            \"key_concepts\": list(sampled_kcs)\n",
    "        })\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# -- 4. Question generation ------------------------------------------------\n",
    "\n",
    "def generate_questions_for_samples(combos, docs, extractions, model=\"gpt-4o\"):  \n",
    "    \"\"\"\n",
    "    For each sampled combo, pick two docs via Jaccard on concept sets,\n",
    "    then call LLM to generate questions.\n",
    "    Returns list of dicts: {'sample': combo, 'questions': [...]}.\n",
    "    \"\"\"\n",
    "    doc_concepts = [set(ex['topics'] + ex['key_concepts']) for ex in extractions]\n",
    "    results = []\n",
    "    max_samples = 5\n",
    "    combos = combos[:max_samples]\n",
    "    for i, combo in enumerate(combos):\n",
    "        combo_id = f\"combo_{i+1}\"\n",
    "        kg = set(combo['topics']) | set(combo['key_concepts'])\n",
    "        sims = []\n",
    "        for idx, dc in enumerate(doc_concepts):\n",
    "            inter = kg & dc\n",
    "            union = kg | dc\n",
    "            sims.append((len(inter) / (len(union) or 1), idx))\n",
    "        sims.sort(reverse=True)\n",
    "        top_idxs = [i for _, i in sims[:2]]\n",
    "        refs = [docs[i] for i in top_idxs]\n",
    "        ref_files = [extractions[i][\"filename\"] for i in top_idxs]\n",
    "\n",
    "        prompt = (\n",
    "            f\"Generate a set of difficult mathematics questions based on the following:\\n\"\n",
    "            f\"\"\"\n",
    "Each question must follow these instructions:\n",
    "Input: Be fully text-based. No figures or diagrams allowed. Avoid any dependence on external media.\n",
    "Model a Physical Scenario: Start from a real-world or idealized setup. Avoid abstract math problems or purely conceptual statements.\n",
    "Target a Solvable Quantity: Ask for a clear symbolic expression of a physical variable (e.g., tension, acceleration, energy).\n",
    "Force Multi-Step Reasoning: Ensure the question involves a sequence of physics laws, transformations, and derivations to reach the answer.\n",
    "Avoid Redundancy: Exclude extraneous details or variables that do not impact the final solution.\n",
    "Be Unique: Do not rephrase standard textbook problems; ensure originality and complexity.\n",
    "Single solution: Expect a single symbolic expression, unambiguous, presented in LaTeX. Multiple equivalent algebraic forms are allowed. No equations or floating-point approximations.\n",
    "Use rigorous, concise phrasing.\n",
    "Avoid colloquial or ambiguous terminology.\n",
    "Units must be consistent; symbols should follow standard notation.\n",
    "\"\"\"\n",
    "            f\"Topics: {combo['topics']}\\n\"\n",
    "            f\"Key Concepts: {combo['key_concepts']}\\n\"\n",
    "            f\"Reference Doc 1:\\n{refs[0]}\\n\"\n",
    "        )\n",
    "        if len(refs) > 1:\n",
    "            prompt += f\"Reference Doc 2:\\n{refs[1]}\\n\"\n",
    "        prompt += \"Return a JSON array of questions.\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful question generator. Return your response as a JSON array of questions.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        try:\n",
    "            # Clean and extract JSON from the output\n",
    "            json_str = extract_json_from_text(content)\n",
    "            questions = json.loads(json_str)\n",
    "            if not isinstance(questions, list):\n",
    "                # If the output is an object with a questions key\n",
    "                if isinstance(questions, dict) and \"questions\" in questions:\n",
    "                    questions = questions[\"questions\"]\n",
    "                else:\n",
    "                    questions = [str(questions)]\n",
    "        except json.JSONDecodeError:\n",
    "            questions = [content]\n",
    "\n",
    "        results.append({\n",
    "            \"id\": combo_id,\n",
    "            \"topics\": combo['topics'],\n",
    "            \"key_concepts\": combo['key_concepts'],\n",
    "            \"reference_files\": ref_files,\n",
    "            \"questions\": questions\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# -- Save outputs to files ------------------------------------------------\n",
    "\n",
    "def save_extractions(extractions, output_file=\"document_extractions.json\"):\n",
    "    \"\"\"Save the extracted topics and key concepts for each document\"\"\"\n",
    "    # Ensure the extractions are serializable (convert sets to lists)\n",
    "    serializable_extractions = []\n",
    "    for ex in extractions:\n",
    "        serializable_extractions.append({\n",
    "            \"filename\": ex[\"filename\"],\n",
    "            \"topics\": list(ex[\"topics\"]),\n",
    "            \"key_concepts\": list(ex[\"key_concepts\"])\n",
    "        })\n",
    "    \n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(serializable_extractions, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved document extractions to {output_file}\")\n",
    "\n",
    "def save_questions_with_topics(questions, output_file=\"questions_with_topics.json\"):\n",
    "    \"\"\"Save the generated questions with their topic combinations\"\"\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(questions, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved questions with topic combinations to {output_file}\")\n",
    "\n",
    "# -- Main Execution --------------------------------------------------------\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Create output directory if it doesn't exist\n",
    "#     output_dir = \"output\"\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "#     # adjust this path to where your .pdf docs live\n",
    "#     docs_dir = \"docs/\"\n",
    "    \n",
    "#     print(\"Loading documents...\")\n",
    "#     docs, filenames = load_docs_from_dir(docs_dir)\n",
    "#     print(f\"Loaded {len(docs)} documents\")\n",
    "    \n",
    "#     if not docs:\n",
    "#         print(\"No documents found. Please check the docs directory.\")\n",
    "#         exit(1)\n",
    "    \n",
    "#     # 1) Extract topics & KCs\n",
    "#     print(\"\\nExtracting topics and key concepts...\")\n",
    "#     extractions = extract_concepts_from_docs(docs, filenames)\n",
    "    \n",
    "#     # Save extractions to file\n",
    "#     save_extractions(extractions, os.path.join(output_dir, \"document_extractions.json\"))\n",
    "    \n",
    "#     # Verify we have valid extractions\n",
    "#     valid_extractions = [ex for ex in extractions if ex[\"topics\"] or ex[\"key_concepts\"]]\n",
    "#     if not valid_extractions:\n",
    "#         print(\"No valid topics or key concepts extracted. Check your data and API responses.\")\n",
    "#         exit(1)\n",
    "    \n",
    "#     # 2) Build graph\n",
    "#     print(\"\\nBuilding concept graph...\")\n",
    "#     G, topic_nodes, kc_nodes = build_concept_graph(extractions)\n",
    "    \n",
    "#     if not topic_nodes:\n",
    "#         print(\"No topics found in the graph. Cannot proceed.\")\n",
    "#         exit(1)\n",
    "    \n",
    "#     # 3) Sample combinations\n",
    "#     print(\"\\nSampling concept combinations...\")\n",
    "#     combos = sample_concept_combinations(G, topic_nodes, kc_nodes, num_samples=10)  # Reduced for testing\n",
    "    \n",
    "#     if not combos:\n",
    "#         print(\"Failed to generate concept combinations.\")\n",
    "#         exit(1)\n",
    "    \n",
    "#     # Save topic combinations\n",
    "#     with open(os.path.join(output_dir, \"topic_combinations.json\"), \"w\") as f:\n",
    "#         json.dump(combos, f, indent=2)\n",
    "    \n",
    "#     # 4) Generate questions\n",
    "#     print(\"\\nGenerating questions for each combination...\")\n",
    "#     q_outputs = generate_questions_for_samples(combos, docs, extractions)\n",
    "    \n",
    "#     # Save questions with topics\n",
    "#     save_questions_with_topics(q_outputs, os.path.join(output_dir, \"questions_with_topics.json\"))\n",
    "    \n",
    "#     # Display results\n",
    "#     print(\"\\n===== GENERATED QUESTIONS =====\")\n",
    "#     for idx, out in enumerate(q_outputs, 1):\n",
    "#         print(f\"\\nSample {idx}:\")\n",
    "#         print(f\"Topics: {out['topics']}\")\n",
    "#         print(f\"Key Concepts: {out['key_concepts']}\")\n",
    "#         print(f\"Reference Files: {out['reference_files']}\")\n",
    "#         print(\"Questions:\")\n",
    "#         for q in out['questions']:\n",
    "#             print(f\" - {q}\")\n",
    "    \n",
    "#     print(f\"\\nAll outputs saved to directory: {output_dir}\") \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create output directory if it doesn't exist\n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = \"output\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # adjust this path to where your .pdf docs live\n",
    "    docs_dir = \"docs/\"\n",
    "    \n",
    "    print(\"Loading documents...\")\n",
    "    docs, filenames = load_docs_from_dir(docs_dir)\n",
    "    print(f\"Loaded {len(docs)} documents\")\n",
    "    document_extractions = []\n",
    "    with open(\"output/document_extractions.json\", \"r\") as f:\n",
    "        document_extractions = json.load(f)\n",
    "    for doc in document_extractions:\n",
    "        print(doc)\n",
    "        break\n",
    "\n",
    "    topic_combinations = []\n",
    "    with open(\"output/topic_combinations.json\", \"r\") as f:\n",
    "        topic_combinations = json.load(f)\n",
    "    for combo in topic_combinations:\n",
    "        print(combo)\n",
    "        break\n",
    "\n",
    "    # generate questions \n",
    "    print(\"Generating questions...\")\n",
    "    q_outputs = generate_questions_for_samples(topic_combinations, docs, document_extractions)\n",
    "\n",
    "    # Save questions with topics\n",
    "    save_questions_with_topics(q_outputs, os.path.join(output_dir, \"questions_with_topics.json\"))\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n===== GENERATED QUESTIONS =====\")\n",
    "    for idx, out in enumerate(q_outputs, 1):\n",
    "        print(f\"\\nSample {idx}:\")\n",
    "        print(f\"Topics: {out['topics']}\")\n",
    "        print(f\"Key Concepts: {out['key_concepts']}\")\n",
    "        print(f\"Reference Files: {out['reference_files']}\")\n",
    "        print(\"Questions:\")\n",
    "        for q in out['questions']:\n",
    "            print(f\" - {q}\")\n",
    "    \n",
    "    print(f\"\\nAll outputs saved to directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
