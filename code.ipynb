{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 documents\n",
      "\n",
      "Extracting topics and key concepts...\n",
      "Raw output: ```json\n",
      "{\n",
      "  \"topics\": [\n",
      "    \"Hierarchy Problem\",\n",
      "    \"Electroweak Symmetry Breaking (EWSB)\",\n",
      "    \"Standard Model and Higgs Boson\",\n",
      "    \"Supersymmetry\",\n",
      "    \"Quantum Field Theory\",\n",
      "    \"High Energy Physics\",\n",
      "    \"Future Colliders\",\n",
      "    \"Naturalness in Physics\"\n",
      "  ],\n",
      "  \"key_concepts\": [\n",
      "    \"Hierarchy Problem: Concerns the large discrepancy between the weak force scale and the Planck scale, needing an explanation beyond the Standard Model.\",\n",
      "    \"Higgs Mechanism: The role and mass of the Higgs boson in electroweak symmetry breaking within the Standard Model.\",\n",
      "    \"Wilsonian Effective Field Theory: A viewpoint addressing the significance of energy scales in quantum field theories inspired by Ken Wilson's work.\",\n",
      "    \"Naturalness: The principle that addresses the fine-tuning of parameters in physics, especially concerning particle masses.\",\n",
      "    \"Supersymmetry (SUSY): A theoretical framework that extends the Standard Model, solving some hierarchy problems by predicting partners for every Standard Model particle.\",\n",
      "    \"Little Hierarchy Problem: The puzzle of why new particles related to EWSB have not yet been observed at the Large Hadron Collider (LHC).\",\n",
      "    \"Role of High Energy Colliders: Future colliders, like a 100 TeV proton collider, are posited as necessary to uncover new physics that current models predict.\",\n",
      "    \"Post-Naturalness Era: A phase in theoretical physics characterized by exploring beyond traditional solutions to the Hierarchy Problem, such as anthropic principles or cosmological considerations.\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "Successfully extracted 8 topics and 8 key concepts from 2505.00694v1.pdf\n",
      "Raw output: ```json\n",
      "{\n",
      "    \"topics\": [\n",
      "        \"Multiplicity Fluctuations in QCD\",\n",
      "        \"Polyakov-KNO Scaling\",\n",
      "        \"Perturbative QCD\",\n",
      "        \"Jet Physics and LHC Experiments\",\n",
      "        \"QCD Predictions vs Experimental Data\",\n",
      "        \"Jet Substructure and Multiplicity Bias\"\n",
      "    ],\n",
      "    \"key_concepts\": [\n",
      "        \"Hadron multiplicity fluctuations in hard processes\",\n",
      "        \"Polyakov-KNO Scaling in particle physics\",\n",
      "        \"Double-logarithmic approximation in QCD\",\n",
      "        \"Multiplicity moments and their perturbative QCD treatment\",\n",
      "        \"Jet hardness scale and its effect on multiplicity\",\n",
      "        \"CMS ellipticity in high-multiplicity jets\",\n",
      "        \"Rattlesnake Effect (RSE) and its implications\",\n",
      "        \"ATLAS and CMS experimental observations in jet physics\",\n",
      "        \"QCD coupling and scaling violation\",\n",
      "        \"Quark and gluon jet comparisons in QCD\",\n",
      "        \"Jet axis determination and substructure analysis\",\n",
      "        \"Analytic perturbative QCD in multiplicity distributions\",\n",
      "        \"Phenomenology of e+e− annihilation and LHC jets\",\n",
      "        \"Laplace transform techniques in solving QCD equations\"\n",
      "    ]\n",
      "}\n",
      "```\n",
      "Successfully extracted 6 topics and 14 key concepts from 2505.00652v1.pdf\n",
      "Raw output: ```json\n",
      "{\n",
      "  \"topics\": [\n",
      "    \"Naturalness in Particle Physics\",\n",
      "    \"Cosmological Constant Problem\",\n",
      "    \"Electroweak Hierarchy Problem\",\n",
      "    \"Strong CP Problem\",\n",
      "    \"Supersymmetry\",\n",
      "    \"Anthropic Reasoning\",\n",
      "    \"Relaxation Mechanisms\",\n",
      "    \"UV/IR Mixing\",\n",
      "    \"Composite Higgs\",\n",
      "    \"Neutral Naturalness\"\n",
      "  ],\n",
      "  \"key_concepts\": [\n",
      "    \"High-energy physics\",\n",
      "    \"Naturalness-based reasoning\",\n",
      "    \"Quadratic divergences\",\n",
      "    \"Symmetries and technical naturalness\",\n",
      "    \"Wilsonian Naturalness\",\n",
      "    \"Landscape of vacua\",\n",
      "    \"Anthropic Principle\",\n",
      "    \"Axion mechanisms\",\n",
      "    \"Weisskopf and Dirac contributions\",\n",
      "    \"Experimental tests for naturalness\",\n",
      "    \"Concepts of UV/IR mixing in quantum gravity\",\n",
      "    \"Implications of the Higgs mass\",\n",
      "    \"Neutral naturalness and twin Higgs models\",\n",
      "    \"Supersymmetric solutions to naturalness problems\",\n",
      "    \"Relaxion mechanism in cosmology\",\n",
      "    \"Hierarchy problems beyond particle physics\",\n",
      "    \"Future experimental exploration in particle physics\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "Successfully extracted 10 topics and 17 key concepts from 2205.05708v1.pdf\n",
      "Saved document extractions to output/document_extractions.json\n",
      "\n",
      "Building concept graph...\n",
      "Graph built with 23 topics and 39 key concepts\n",
      "\n",
      "Sampling concept combinations...\n",
      "Sampling from 23 topics\n",
      "\n",
      "Generating questions for each combination...\n",
      "Saved questions with topic combinations to output/questions_with_topics.json\n",
      "\n",
      "===== GENERATED QUESTIONS =====\n",
      "\n",
      "Sample 1:\n",
      "Topics: ['Electroweak Hierarchy Problem', 'Cosmological Constant Problem', 'UV/IR Mixing']\n",
      "Key Concepts: ['Future experimental exploration in particle physics', 'Experimental tests for naturalness', 'Supersymmetric solutions to naturalness problems', 'Wilsonian Naturalness', 'Anthropic Principle']\n",
      "Reference Files: ['2205.05708v1.pdf', '2505.00652v1.pdf']\n",
      "Questions:\n",
      " - What are the main challenges in resolving the cosmological constant problem, and how do current theories attempt to address these challenges?\n",
      " - How does the electroweak hierarchy problem highlight tensions between naturalness and experimental evidence observed at the LHC?\n",
      " - Can you explain the concept of UV/IR mixing in quantum gravity and its potential implications for naturalness problems?\n",
      " - What are the key differences between the standard approach of Wilsonian Naturalness and alternative concepts like the Anthropic Principle in addressing naturalness problems?\n",
      " - In the context of the electroweak hierarchy problem, how does supersymmetry aim to resolve issues of naturalness, and what are the current experimental challenges in observing it?\n",
      " - How might future experimental exploration in particle physics help test theories related to the strong CP problem, and what role does the hypothetical axion play in this?\n",
      " - What are some of the experimental signatures that distinguish models of neutral naturalness from those based on supersymmetry?\n",
      " - Discuss the role of the anomalous dimension in multiplicity fluctuation models and its relevance to naturalness arguments in high-energy physics.\n",
      " - How does the relaxion mechanism propose to solve the electroweak hierarchy problem, and what are the cosmological implications of this approach?\n",
      " - What are the theoretical implications and experimental challenges of detecting a cosmological constant that is dramatically smaller than predicted by quantum field theories?\n",
      " - Explain the significance of P-KNO scaling in high-energy particle interactions and its relationship to fundamental concepts in QCD.\n",
      " - How do current measurements of particle multiplicity distributions in high-energy collisions inform us about the underlying naturalness problems in particle physics?\n",
      " - What lessons can be drawn from the historical successes and failures of naturalness-based reasoning in high-energy physics?\n",
      " - Discuss the potential impact of refined experimental precision, such as measurements from the LHC or future accelerators, on the ongoing debates regarding naturalness in particle physics.\n",
      " - In what ways could future theoretical breakthroughs in quantum field theory alter the landscape of naturalness problems and their proposed solutions?\n",
      "\n",
      "Sample 2:\n",
      "Topics: ['Quantum Field Theory', 'Standard Model and Higgs Boson', 'Supersymmetry']\n",
      "Key Concepts: ['Post-Naturalness Era: A phase in theoretical physics characterized by exploring beyond traditional solutions to the Hierarchy Problem, such as anthropic principles or cosmological considerations.', 'Higgs Mechanism: The role and mass of the Higgs boson in electroweak symmetry breaking within the Standard Model.']\n",
      "Reference Files: ['2505.00694v1.pdf', '2205.05708v1.pdf']\n",
      "Questions:\n",
      " - What are the key differences between the Post-Naturalness Era and traditional approaches to the Hierarchy Problem?\n",
      " - How does the Higgs Mechanism contribute to electroweak symmetry breaking in the context of the Standard Model?\n",
      " - Explain how supersymmetry addresses the Hierarchy Problem. What role do superpartners play in this context?\n",
      " - What are the implications of the Post-Naturalness Era for future high-energy collider experiments?\n",
      " - How do composite Higgs models propose to solve the Hierarchy Problem, and what are their main experimental signatures?\n",
      " - Compare and contrast the Wilsonian point of view with other perspectives on the Hierarchy Problem, focusing on the treatment of renormalization.\n",
      " - Discuss the anthropic principle as a potential solution to the Hierarchy Problem. What are its strengths and weaknesses?\n",
      " - What are the main challenges faced by models attempting to solve the Strong CP Problem without relying on the Peccei-Quinn symmetry?\n",
      " - How does the concept of 'Little Hierarchy Problem' differ from the main Hierarchy Problem, and what solutions have been proposed?\n",
      " - Evaluate the effectiveness of recent theoretical approaches that explore UV/IR mixing in addressing the cosmological constant problem.\n",
      " - What insights do the TASI lectures provide into modern views on the hierarchy problem and naturalness?\n",
      " - How do theories incorporating additional dimensions or composite states impact predictions for scalar particle masses?\n",
      " - Explore the experimental bounds and implications of parity-based solutions to the strong CP problem.\n",
      " - In what ways do quantum gravity considerations influence the hierarchy problem and the naturalness of the Higgs mass?\n",
      " - What are the potential consequences of assuming the Higgs field has an anthropically determined vacuum expectation value?\n",
      " - Illustrate the role of UV/IR mixing in providing potential solutions to the hierarchy problem, citing recent theoretical findings.\n",
      " - How might future discoveries in quantum field theory, such as non-commutative field theories, impact the current understanding of the hierarchy problem?\n",
      " - What role do finite size effects and quantum instability play in the naturalness issues related to the Higgs boson?\n",
      "\n",
      "Sample 3:\n",
      "Topics: ['Electroweak Hierarchy Problem', 'UV/IR Mixing']\n",
      "Key Concepts: ['Hierarchy problems beyond particle physics', 'Landscape of vacua', 'Anthropic Principle', 'Implications of the Higgs mass']\n",
      "Reference Files: ['2205.05708v1.pdf', '2505.00652v1.pdf']\n",
      "Questions:\n",
      " - How does the electroweak hierarchy problem differ from the cosmological constant problem in terms of scale and impact on the universe?\n",
      " - What role does the anthropic principle play in resolving hierarchy problems in physics, and how does it relate to the landscape of vacua?\n",
      " - Can you explain the concept of UV/IR mixing and its implications for naturalness problems in particle physics?\n",
      " - In what ways do UV corrections impact the Higgs mass, and how is this related to the electroweak hierarchy problem?\n",
      " - How do solutions to the strong CP problem differ in terms of naturalness and anthropic reasoning?\n",
      " - What are the recent developments in addressing the cosmological constant problem using relaxation mechanisms?\n",
      " - How does technical naturalness connect with 't Hooft naturalness, and why are both concepts significant in solving naturalness problems?\n",
      " - In the context of the Higgs boson, how do naturalness arguments determine the expected scale for new physics?\n",
      " - Discuss the significance of multiplicity fluctuations in perturbative QCD and their relation to high-multiplicity puzzles observed at LHC.\n",
      " - What challenges and novel approaches exist for addressing the electroweak hierarchy problem through discrete symmetries like neutral naturalness?\n",
      " - How might quantum gravity influence our understanding and approach to solving naturalness issues like the hierarchy problem?\n",
      " - What insights can the Noncommutative Quantum Field Theory (NCQFT) provide in addressing naturalness problems?\n",
      " - How do recent theoretical proposals leverage UV/IR mixing to confront long-standing naturalness problems in theoretical physics?\n",
      " - What would be the theoretical implications if the solutions to the cosmological constant and strong CP problems were found to be correlated?\n",
      " - Can recent phenomenological studies of jets at high energy particle colliders inform solutions to the hierarchy problem?\n",
      "\n",
      "Sample 4:\n",
      "Topics: ['Multiplicity Fluctuations in QCD', 'Jet Substructure and Multiplicity Bias']\n",
      "Key Concepts: ['Jet hardness scale and its effect on multiplicity', 'Laplace transform techniques in solving QCD equations', 'Phenomenology of e+e− annihilation and LHC jets', 'ATLAS and CMS experimental observations in jet physics', 'Rattlesnake Effect (RSE) and its implications']\n",
      "Reference Files: ['2505.00652v1.pdf', '2205.05708v1.pdf']\n",
      "Questions:\n",
      " - What role do multiplicity fluctuations play in understanding QCD processes and how are they analyzed in e+e− annihilation and LHC jets?\n",
      " - Explain the importance of the jet hardness scale in relation to multiplicity. How is this concept applied experimentally in ATLAS and CMS observations?\n",
      " - How are Laplace transform techniques employed in solving QCD equations relating to multiplicity fluctuations?\n",
      " - In the context of QCD, what is the significance of high-multiplicity fluctuations in jets and how do they relate to the Polyakov-KNO scaling?\n",
      " - What are the challenges and solutions associated with interpreting data from ATLAS and CMS experiments regarding high-multiplicity tails in jet physics?\n",
      " - Discuss the 'Rattlesnake Effect' (RSE) mentioned in the context of high-multiplicity fluctuations and its theoretical implications.\n",
      " - How does the study of multiplicity anomalous dimensions aid in the understanding of QCD phenomena?\n",
      " - Explore the role of quark jets versus gluon jets in multiplicity studies and their comparative effects on experimental results.\n",
      " - How do e+e− annihilation processes contribute to our understanding of multiplicity distributions, and what are the key findings from TASSO and DELPHI experiments?\n",
      " - What is the relevance of the 'Jet Substructure and Multiplicity Bias' concept in LHC jet analysis?\n",
      " - In what ways do the experimental findings from CMS regarding ellipticity and angular correlations contribute to our understanding of jet physics?\n",
      "\n",
      "Sample 5:\n",
      "Topics: ['Quantum Field Theory', 'Standard Model and Higgs Boson', 'Supersymmetry']\n",
      "Key Concepts: ['Post-Naturalness Era: A phase in theoretical physics characterized by exploring beyond traditional solutions to the Hierarchy Problem, such as anthropic principles or cosmological considerations.', 'Role of High Energy Colliders: Future colliders, like a 100 TeV proton collider, are posited as necessary to uncover new physics that current models predict.', \"Wilsonian Effective Field Theory: A viewpoint addressing the significance of energy scales in quantum field theories inspired by Ken Wilson's work.\", 'Supersymmetry (SUSY): A theoretical framework that extends the Standard Model, solving some hierarchy problems by predicting partners for every Standard Model particle.']\n",
      "Reference Files: ['2505.00694v1.pdf', '2205.05708v1.pdf']\n",
      "Questions:\n",
      " - What is the role of the Wilsonian effective field theory in addressing the Hierarchy Problem in quantum field theories?\n",
      " - How might the post-naturalness era influence our understanding of supersymmetry in theoretical physics?\n",
      " - What experimental data from high-energy colliders is necessary to further explore the Standard Model and Higgs Boson's role in physics?\n",
      " - In what ways do supersymmetric models address the Hierarchy Problem, and how do they predict new particles?\n",
      " - What are some potential solutions to the Little Hierarchy Problem according to current theoretical models?\n",
      " - How does the mechanism of electroweak symmetry breaking challenge the concept of hierarchy within the Standard Model?\n",
      " - Can we develop a comprehensive understanding of the spontaneous symmetry breaking by relating it to known physical models, such as superconductivity?\n",
      " - What new theoretical insights could emerge from the construction of a 100 TeV proton collider regarding the Hierarchy Problem?\n",
      " - How does the idea of the Hierarchy Problem differ in various models, such as the Standard Model compared to supersymmetric models?\n",
      " - What are the implications of considering high-energy colliders necessary to complete the exploration of new particles related to the Hierarchy Problem?\n",
      "\n",
      "All outputs saved to directory: output\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "import pdfplumber\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key from environment variable\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# -- Document Loading ------------------------------------------------------\n",
    "\n",
    "def load_docs_from_dir(dir_path):\n",
    "    \"\"\"\n",
    "    Load all .pdf files from a directory and extract their full text.\n",
    "    Returns a list of strings, one per PDF.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    filenames = []\n",
    "    for filepath in glob.glob(os.path.join(dir_path, '*.pdf')):\n",
    "        try:\n",
    "            text_pages = []\n",
    "            with pdfplumber.open(filepath) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    text_pages.append(page.extract_text() or \"\")\n",
    "            full_text = \"\\n\".join(text_pages)\n",
    "            docs.append(full_text)\n",
    "            filenames.append(os.path.basename(filepath))\n",
    "        except Exception as e:\n",
    "            # skip unreadable PDFs\n",
    "            print(f\"Warning: could not load {filepath}: {e}\")\n",
    "    return docs, filenames\n",
    "\n",
    "# -- 1. Extract topics and key concepts -----------------------------------\n",
    "\n",
    "def extract_json_from_text(text):\n",
    "    \"\"\"Helper function to extract JSON content from markdown code blocks or raw text\"\"\"\n",
    "    # Try to extract JSON from markdown code blocks\n",
    "    json_match = re.search(r'```(?:json)?\\s*(.*?)\\s*```', text, re.DOTALL)\n",
    "    if json_match:\n",
    "        return json_match.group(1).strip()\n",
    "    \n",
    "    # If no code blocks, try to find JSON-like structures\n",
    "    json_match = re.search(r'(\\{.*\\})', text, re.DOTALL)\n",
    "    if json_match:\n",
    "        return json_match.group(1).strip()\n",
    "    \n",
    "    # Return original text if no JSON structure found\n",
    "    return text\n",
    "\n",
    "def extract_concepts_from_docs(doc_texts, filenames, model=\"gpt-4o\"):  \n",
    "    \"\"\"\n",
    "    Call OpenAI API to extract topics and key concepts from each document.\n",
    "    Returns list of dicts with keys 'topics' and 'key_concepts'.\n",
    "    \"\"\"\n",
    "    extractions = []\n",
    "    for i, text in enumerate(doc_texts):\n",
    "        filename = filenames[i] if i < len(filenames) else f\"doc_{i}\"\n",
    "        \n",
    "        prompt = (\n",
    "            \"Extract high-level topics and key concepts from the following document. \"\n",
    "            f\"Return JSON with keys 'topics' and 'key_concepts'.\\n\\n{text}\"\n",
    "        )\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert summarizer. Return your response as a JSON object with 'topics' and 'key_concepts' as arrays.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        output = response.choices[0].message.content\n",
    "        print(\"Raw output:\", output)\n",
    "        \n",
    "        try:\n",
    "            # Clean and extract JSON from the output\n",
    "            json_str = extract_json_from_text(output)\n",
    "            data = json.loads(json_str)\n",
    "            \n",
    "            # Ensure we have the expected keys\n",
    "            if \"topics\" in data and \"key_concepts\" in data:\n",
    "                extraction = {\n",
    "                    \"filename\": filename,\n",
    "                    \"topics\": data[\"topics\"],\n",
    "                    \"key_concepts\": data[\"key_concepts\"]\n",
    "                }\n",
    "                extractions.append(extraction)\n",
    "                print(f\"Successfully extracted {len(data['topics'])} topics and {len(data['key_concepts'])} key concepts from {filename}\")\n",
    "            else:\n",
    "                print(f\"Warning: Parsed JSON doesn't have expected keys: {data.keys()}\")\n",
    "                extractions.append({\n",
    "                    \"filename\": filename,\n",
    "                    \"topics\": [], \n",
    "                    \"key_concepts\": []\n",
    "                })\n",
    "                \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Failed to parse JSON: {e}\")\n",
    "            extractions.append({\n",
    "                \"filename\": filename,\n",
    "                \"topics\": [], \n",
    "                \"key_concepts\": []\n",
    "            })\n",
    "    \n",
    "    return extractions\n",
    "\n",
    "# -- 2. Construct the concept graph ----------------------------------------\n",
    "\n",
    "def build_concept_graph(extractions, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Build a unified graph G where nodes are topics + key concepts and\n",
    "    edges weighted by log(freq+eps) based on co-occurrence in docs.\n",
    "    Returns: G (nx.Graph), topic_nodes, kc_nodes\n",
    "    \"\"\"\n",
    "    freq = Counter()\n",
    "    all_topics, all_kcs = set(), set()\n",
    "\n",
    "    for ex in extractions:\n",
    "        nodes = ex[\"topics\"] + ex[\"key_concepts\"]\n",
    "        for i in range(len(nodes)):\n",
    "            for j in range(i+1, len(nodes)):\n",
    "                u, v = sorted((nodes[i], nodes[j]))\n",
    "                freq[(u, v)] += 1\n",
    "        all_topics.update(ex[\"topics\"])\n",
    "        all_kcs.update(ex[\"key_concepts\"])\n",
    "\n",
    "    G = nx.Graph()\n",
    "    for (u, v), f in freq.items():\n",
    "        weight = math.log(f + eps)\n",
    "        G.add_edge(u, v, weight=weight)\n",
    "\n",
    "    print(f\"Graph built with {len(all_topics)} topics and {len(all_kcs)} key concepts\")\n",
    "    return G, all_topics, all_kcs\n",
    "\n",
    "# -- Helpers for sampling --------------------------------------------------\n",
    "\n",
    "def softmax(weights):\n",
    "    exps = [math.exp(w) for w in weights]\n",
    "    s = sum(exps) or 1.0\n",
    "    return [e/s for e in exps]\n",
    "\n",
    "\n",
    "def random_walk(G, start, steps):\n",
    "    \"\"\"\n",
    "    Random walk on graph G for given steps from 'start',\n",
    "    with transition probabilities via softmax over edge weights.\n",
    "    \"\"\"\n",
    "    path = [start]\n",
    "    current = start\n",
    "    for _ in range(steps):\n",
    "        nbrs = list(G[current])\n",
    "        if not nbrs:\n",
    "            break\n",
    "        weights = [G[current][n]['weight'] for n in nbrs]\n",
    "        probs = softmax(weights)\n",
    "        current = random.choices(nbrs, probs)[0]\n",
    "        path.append(current)\n",
    "    return path\n",
    "\n",
    "# -- 3. Concept combination sampling ---------------------------------------\n",
    "\n",
    "def sample_concept_combinations(\n",
    "    G, topic_nodes, kc_nodes,\n",
    "    num_samples=100,\n",
    "    topic_walk_steps=(1, 2),\n",
    "    kc_walk_steps=(3, 4)\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate sampled sets of topics and key concepts via multi-stage random walks.\n",
    "    Returns list of dicts: {'topics': set, 'key_concepts': set}\n",
    "    \"\"\"\n",
    "    # Safety check\n",
    "    if not topic_nodes:\n",
    "        print(\"Error: No topics found. Cannot sample combinations.\")\n",
    "        return []\n",
    "        \n",
    "    G_topic = G.subgraph(topic_nodes)\n",
    "    G_topic_kc = G.subgraph(topic_nodes | kc_nodes)\n",
    "    G_kc = G.subgraph(kc_nodes)\n",
    "    samples = []\n",
    "\n",
    "    topics_list = list(topic_nodes)\n",
    "    print(f\"Sampling from {len(topics_list)} topics\")\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        t0 = random.choice(topics_list)\n",
    "        t_steps = random.choice(topic_walk_steps)\n",
    "        topic_path = random_walk(G_topic, t0, t_steps)\n",
    "        sampled_topics = set(topic_path)\n",
    "\n",
    "        kc_cands = [nbr for t in sampled_topics for nbr in G_topic_kc[t] if nbr in kc_nodes]\n",
    "        if kc_cands:\n",
    "            k0 = random.choice(kc_cands)\n",
    "            k_steps = random.choice(kc_walk_steps)\n",
    "            kc_path = random_walk(G_kc, k0, k_steps)\n",
    "            sampled_kcs = set(kc_path)\n",
    "        else:\n",
    "            sampled_kcs = set()\n",
    "\n",
    "        samples.append({\n",
    "            \"topics\": list(sampled_topics),  # Convert sets to lists for JSON serialization\n",
    "            \"key_concepts\": list(sampled_kcs)\n",
    "        })\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# -- 4. Question generation ------------------------------------------------\n",
    "\n",
    "def generate_questions_for_samples(combos, docs, extractions, model=\"gpt-4o\"):  \n",
    "    \"\"\"\n",
    "    For each sampled combo, pick two docs via Jaccard on concept sets,\n",
    "    then call LLM to generate questions.\n",
    "    Returns list of dicts: {'sample': combo, 'questions': [...]}.\n",
    "    \"\"\"\n",
    "    doc_concepts = [set(ex['topics'] + ex['key_concepts']) for ex in extractions]\n",
    "    results = []\n",
    "    max_samples = 5\n",
    "    combos = combos[:max_samples]\n",
    "    for i, combo in enumerate(combos):\n",
    "        combo_id = f\"combo_{i+1}\"\n",
    "        kg = set(combo['topics']) | set(combo['key_concepts'])\n",
    "        sims = []\n",
    "        for idx, dc in enumerate(doc_concepts):\n",
    "            inter = kg & dc\n",
    "            union = kg | dc\n",
    "            sims.append((len(inter) / (len(union) or 1), idx))\n",
    "        sims.sort(reverse=True)\n",
    "        top_idxs = [i for _, i in sims[:2]]\n",
    "        refs = [docs[i] for i in top_idxs]\n",
    "        ref_files = [extractions[i][\"filename\"] for i in top_idxs]\n",
    "\n",
    "        prompt = (\n",
    "            f\"Generate a set of difficult mathematics questions based on the following:\\n\"\n",
    "            f\"Topics: {combo['topics']}\\n\"\n",
    "            f\"Key Concepts: {combo['key_concepts']}\\n\"\n",
    "            f\"Reference Doc 1:\\n{refs[0]}\\n\"\n",
    "        )\n",
    "        if len(refs) > 1:\n",
    "            prompt += f\"Reference Doc 2:\\n{refs[1]}\\n\"\n",
    "        prompt += \"Return a JSON array of questions.\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful question generator. Return your response as a JSON array of questions.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        try:\n",
    "            # Clean and extract JSON from the output\n",
    "            json_str = extract_json_from_text(content)\n",
    "            questions = json.loads(json_str)\n",
    "            if not isinstance(questions, list):\n",
    "                # If the output is an object with a questions key\n",
    "                if isinstance(questions, dict) and \"questions\" in questions:\n",
    "                    questions = questions[\"questions\"]\n",
    "                else:\n",
    "                    questions = [str(questions)]\n",
    "        except json.JSONDecodeError:\n",
    "            questions = [content]\n",
    "\n",
    "        results.append({\n",
    "            \"id\": combo_id,\n",
    "            \"topics\": combo['topics'],\n",
    "            \"key_concepts\": combo['key_concepts'],\n",
    "            \"reference_files\": ref_files,\n",
    "            \"questions\": questions\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# -- Save outputs to files ------------------------------------------------\n",
    "\n",
    "def save_extractions(extractions, output_file=\"document_extractions.json\"):\n",
    "    \"\"\"Save the extracted topics and key concepts for each document\"\"\"\n",
    "    # Ensure the extractions are serializable (convert sets to lists)\n",
    "    serializable_extractions = []\n",
    "    for ex in extractions:\n",
    "        serializable_extractions.append({\n",
    "            \"filename\": ex[\"filename\"],\n",
    "            \"topics\": list(ex[\"topics\"]),\n",
    "            \"key_concepts\": list(ex[\"key_concepts\"])\n",
    "        })\n",
    "    \n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(serializable_extractions, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved document extractions to {output_file}\")\n",
    "\n",
    "def save_questions_with_topics(questions, output_file=\"questions_with_topics.json\"):\n",
    "    \"\"\"Save the generated questions with their topic combinations\"\"\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(questions, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved questions with topic combinations to {output_file}\")\n",
    "\n",
    "# -- Main Execution --------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = \"output\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # adjust this path to where your .pdf docs live\n",
    "    docs_dir = \"docs/\"\n",
    "    \n",
    "    print(\"Loading documents...\")\n",
    "    docs, filenames = load_docs_from_dir(docs_dir)\n",
    "    print(f\"Loaded {len(docs)} documents\")\n",
    "    \n",
    "    if not docs:\n",
    "        print(\"No documents found. Please check the docs directory.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # 1) Extract topics & KCs\n",
    "    print(\"\\nExtracting topics and key concepts...\")\n",
    "    extractions = extract_concepts_from_docs(docs, filenames)\n",
    "    \n",
    "    # Save extractions to file\n",
    "    save_extractions(extractions, os.path.join(output_dir, \"document_extractions.json\"))\n",
    "    \n",
    "    # Verify we have valid extractions\n",
    "    valid_extractions = [ex for ex in extractions if ex[\"topics\"] or ex[\"key_concepts\"]]\n",
    "    if not valid_extractions:\n",
    "        print(\"No valid topics or key concepts extracted. Check your data and API responses.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # 2) Build graph\n",
    "    print(\"\\nBuilding concept graph...\")\n",
    "    G, topic_nodes, kc_nodes = build_concept_graph(extractions)\n",
    "    \n",
    "    if not topic_nodes:\n",
    "        print(\"No topics found in the graph. Cannot proceed.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # 3) Sample combinations\n",
    "    print(\"\\nSampling concept combinations...\")\n",
    "    combos = sample_concept_combinations(G, topic_nodes, kc_nodes, num_samples=10)  # Reduced for testing\n",
    "    \n",
    "    if not combos:\n",
    "        print(\"Failed to generate concept combinations.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Save topic combinations\n",
    "    with open(os.path.join(output_dir, \"topic_combinations.json\"), \"w\") as f:\n",
    "        json.dump(combos, f, indent=2)\n",
    "    \n",
    "    # 4) Generate questions\n",
    "    print(\"\\nGenerating questions for each combination...\")\n",
    "    q_outputs = generate_questions_for_samples(combos, docs, extractions)\n",
    "    \n",
    "    # Save questions with topics\n",
    "    save_questions_with_topics(q_outputs, os.path.join(output_dir, \"questions_with_topics.json\"))\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n===== GENERATED QUESTIONS =====\")\n",
    "    for idx, out in enumerate(q_outputs, 1):\n",
    "        print(f\"\\nSample {idx}:\")\n",
    "        print(f\"Topics: {out['topics']}\")\n",
    "        print(f\"Key Concepts: {out['key_concepts']}\")\n",
    "        print(f\"Reference Files: {out['reference_files']}\")\n",
    "        print(\"Questions:\")\n",
    "        for q in out['questions']:\n",
    "            print(f\" - {q}\")\n",
    "    \n",
    "    print(f\"\\nAll outputs saved to directory: {output_dir}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
